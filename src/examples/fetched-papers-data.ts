/**
 * Fetched papers for Gaussian Splatting knowledge graph
 * 
 * Generated from Semantic Scholar API
 * Source: scripts/fetch-papers.ts
 * 
 * NOTE: These papers have abstracts but not full text.
 * In production, you would:
 * 1. Download PDFs from arXiv using the arxiv_id field
 * 2. Parse PDFs to extract full text
 * 3. Use the full text for more comprehensive entity extraction
 * 
 * For now, the full_text field contains a synthetic version
 * based on the abstract. Entity extraction will work but will
 * be less rich than with actual full papers.
 */

export interface Paper {
  title: string;
  abstract: string;
  full_text: string;
  authors: string[];
  arxiv_id: string | null;
  publication_date: Date;
  venue: string;
  doi: string | null;
  citation_count: number;
}

export const fetchedPapers: Paper[] = [
  {
    title: "Neural Point Catacaustics for Novel-View Synthesis of Reflections",
    abstract: "View-dependent effects such as reflections pose a substantial challenge for image-based and neural rendering algorithms. Above all, curved reflectors are particularly hard, as they lead to highly non-linear reflection flows as the camera moves. We introduce a new point-based representation to compute Neural Point Catacaustics allowing novel-view synthesis of scenes with curved reflectors, from a set of casually-captured input photos. At the core of our method is a neural warp field that models catacaustic trajectories of reflections, so complex specular effects can be rendered using efficient point splatting in conjunction with a neural renderer. One of our key contributions is the explicit representation of reflections with a reflection point cloud which is displaced by the neural warp field, and a primary point cloud which is optimized to represent the rest of the scene. After a short manual annotation step, our approach allows interactive high-quality renderings of novel views with accurate reflection flow. Additionally, the explicit representation of reflection flow supports several forms of scene manipulation in captured scenes, such as reflection editing, cloning of specular objects, reflection tracking across views, and comfortable stereo viewing. We provide the source code and other supplemental material on https://repo-sam.inria.fr/fungraph/neural_catacaustics/",
    full_text: "# Neural Point Catacaustics for Novel-View Synthesis of Reflections\n\n## Abstract\n\nView-dependent effects such as reflections pose a substantial challenge for image-based and neural rendering algorithms. Above all, curved reflectors are particularly hard, as they lead to highly non-linear reflection flows as the camera moves. We introduce a new point-based representation to compute Neural Point Catacaustics allowing novel-view synthesis of scenes with curved reflectors, from a set of casually-captured input photos. At the core of our method is a neural warp field that models catacaustic trajectories of reflections, so complex specular effects can be rendered using efficient point splatting in conjunction with a neural renderer. One of our key contributions is the explicit representation of reflections with a reflection point cloud which is displaced by the neural warp field, and a primary point cloud which is optimized to represent the rest of the scene. After a short manual annotation step, our approach allows interactive high-quality renderings of novel views with accurate reflection flow. Additionally, the explicit representation of reflection flow supports several forms of scene manipulation in captured scenes, such as reflection editing, cloning of specular objects, reflection tracking across views, and comfortable stereo viewing. We provide the source code and other supplemental material on https://repo-sam.inria.fr/fungraph/neural_catacaustics/\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nView-dependent effects such as reflections pose a substantial challenge for image-based and neural rendering algorithms.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Georgios Kopanas","Thomas Leimkühler","G. Rainer","Clément Jambon","G. Drettakis"],
    arxiv_id: "2301.01087",
    publication_date: new Date("2022-11-30T00:00:00.000Z"),
    venue: "ACM Transactions on Graphics",
    doi: "10.1145/3550454.3555497",
    citation_count: 100,
  },
  {
    title: "Variable Bitrate Neural Fields",
    abstract: "Neural approximations of scalar- and vector fields, such as signed distance functions and radiance fields, have emerged as accurate, high-quality representations. State-of-the-art results are obtained by conditioning a neural approximation with a lookup from trainable feature grids [Liu et al. 2020; Martel et al. 2021; Müller et al. 2022; Takikawa et al. 2021] that take on part of the learning task and allow for smaller, more efficient neural networks. Unfortunately, these feature grids usually come at the cost of significantly increased memory consumption compared to stand-alone neural network models. We present a dictionary method for compressing such feature grids, reducing their memory consumption by up to 100 × and permitting a multiresolution representation which can be useful for out-of-core streaming. We formulate the dictionary optimization as a vector-quantized auto-decoder problem which lets us learn end-to-end discrete neural representations in a space where no direct supervision is available and with dynamic topology and structure. Our source code is available at https://github.com/nv-tlabs/vqad.",
    full_text: "# Variable Bitrate Neural Fields\n\n## Abstract\n\nNeural approximations of scalar- and vector fields, such as signed distance functions and radiance fields, have emerged as accurate, high-quality representations. State-of-the-art results are obtained by conditioning a neural approximation with a lookup from trainable feature grids [Liu et al. 2020; Martel et al. 2021; Müller et al. 2022; Takikawa et al. 2021] that take on part of the learning task and allow for smaller, more efficient neural networks. Unfortunately, these feature grids usually come at the cost of significantly increased memory consumption compared to stand-alone neural network models. We present a dictionary method for compressing such feature grids, reducing their memory consumption by up to 100 × and permitting a multiresolution representation which can be useful for out-of-core streaming. We formulate the dictionary optimization as a vector-quantized auto-decoder problem which lets us learn end-to-end discrete neural representations in a space where no direct supervision is available and with dynamic topology and structure. Our source code is available at https://github.com/nv-tlabs/vqad.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nNeural approximations of scalar- and vector fields, such as signed distance functions and radiance fields, have emerged as accurate, high-quality representations.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Towaki Takikawa","Alex Evans","Jonathan Tremblay","T. Müller","M. McGuire","Alec Jacobson","S. Fidler"],
    arxiv_id: "2206.07707",
    publication_date: new Date("2022-06-15T00:00:00.000Z"),
    venue: "International Conference on Computer Graphics and Interactive Techniques",
    doi: "10.1145/3528233.3530727",
    citation_count: 141,
  },
  {
    title: "Differentiable Point-Based Radiance Fields for Efficient View Synthesis",
    abstract: "We propose a differentiable rendering algorithm for efficient novel view synthesis. By departing from volume-based representations in favor of a learned point representation, we improve on existing methods more than an order of magnitude in memory and runtime, both in training and inference. The method begins with a uniformly-sampled random point cloud and learns per-point position and view-dependent appearance, using a differentiable splat-based renderer to train the model to reproduce a set of input training images with the given pose. Our method is up to 300 × faster than NeRF in both training and inference, with only a marginal sacrifice in quality, while using less than 10 MB of memory for a static scene. For dynamic scenes, our method trains two orders of magnitude faster than STNeRF and renders at a near interactive rate, while maintaining high image quality and temporal coherence even without imposing any temporal-coherency regularizers.",
    full_text: "# Differentiable Point-Based Radiance Fields for Efficient View Synthesis\n\n## Abstract\n\nWe propose a differentiable rendering algorithm for efficient novel view synthesis. By departing from volume-based representations in favor of a learned point representation, we improve on existing methods more than an order of magnitude in memory and runtime, both in training and inference. The method begins with a uniformly-sampled random point cloud and learns per-point position and view-dependent appearance, using a differentiable splat-based renderer to train the model to reproduce a set of input training images with the given pose. Our method is up to 300 × faster than NeRF in both training and inference, with only a marginal sacrifice in quality, while using less than 10 MB of memory for a static scene. For dynamic scenes, our method trains two orders of magnitude faster than STNeRF and renders at a near interactive rate, while maintaining high image quality and temporal coherence even without imposing any temporal-coherency regularizers.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nWe propose a differentiable rendering algorithm for efficient novel view synthesis.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Qian Zhang","Seung-Hwan Baek","S. Rusinkiewicz","Felix Heide"],
    arxiv_id: "2205.14330",
    publication_date: new Date("2022-05-28T00:00:00.000Z"),
    venue: "ACM SIGGRAPH Conference and Exhibition on Computer Graphics and Interactive Techniques in Asia",
    doi: "10.1145/3550469.3555413",
    citation_count: 106,
  },
  {
    title: "TensoRF: Tensorial Radiance Fields",
    abstract: "We present TensoRF, a novel approach to model and reconstruct radiance fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a scene as a 4D tensor, which represents a 3D voxel grid with per-voxel multi-channel features. Our central idea is to factorize the 4D scene tensor into multiple compact low-rank tensor components. We demonstrate that applying traditional CP decomposition -- that factorizes tensors into rank-one components with compact vectors -- in our framework leads to improvements over vanilla NeRF. To further boost performance, we introduce a novel vector-matrix (VM) decomposition that relaxes the low-rank constraints for two modes of a tensor and factorizes tensors into compact vector and matrix factors. Beyond superior rendering quality, our models with CP and VM decompositions lead to a significantly lower memory footprint in comparison to previous and concurrent works that directly optimize per-voxel features. Experimentally, we demonstrate that TensoRF with CP decomposition achieves fast reconstruction (<30 min) with better rendering quality and even a smaller model size (<4 MB) compared to NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality and outperforms previous state-of-the-art methods, while reducing the reconstruction time (<10 min) and retaining a compact model size (<75 MB).",
    full_text: "# TensoRF: Tensorial Radiance Fields\n\n## Abstract\n\nWe present TensoRF, a novel approach to model and reconstruct radiance fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a scene as a 4D tensor, which represents a 3D voxel grid with per-voxel multi-channel features. Our central idea is to factorize the 4D scene tensor into multiple compact low-rank tensor components. We demonstrate that applying traditional CP decomposition -- that factorizes tensors into rank-one components with compact vectors -- in our framework leads to improvements over vanilla NeRF. To further boost performance, we introduce a novel vector-matrix (VM) decomposition that relaxes the low-rank constraints for two modes of a tensor and factorizes tensors into compact vector and matrix factors. Beyond superior rendering quality, our models with CP and VM decompositions lead to a significantly lower memory footprint in comparison to previous and concurrent works that directly optimize per-voxel features. Experimentally, we demonstrate that TensoRF with CP decomposition achieves fast reconstruction (<30 min) with better rendering quality and even a smaller model size (<4 MB) compared to NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality and outperforms previous state-of-the-art methods, while reducing the reconstruction time (<10 min) and retaining a compact model size (<75 MB).\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nWe present TensoRF, a novel approach to model and reconstruct radiance fields.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Anpei Chen","Zexiang Xu","Andreas Geiger","Jingyi Yu","Hao Su"],
    arxiv_id: "2203.09517",
    publication_date: new Date("2022-03-17T00:00:00.000Z"),
    venue: "European Conference on Computer Vision",
    doi: "10.48550/arXiv.2203.09517",
    citation_count: 1513,
  },
  {
    title: "Point-NeRF: Point-based Neural Radiance Fields",
    abstract: "Volumetric neural rendering methods like NeRF [34] generate high-quality view synthesis results but are optimized per-scene leading to prohibitive reconstruction time. On the other hand, deep multi-view stereo methods can quickly reconstruct scene geometry via direct network inference. Point-NeRF combines the advantages of these two approaches by using neural 3D point clouds, with associated neural features, to model a radiance field. Point-NeRF can be rendered efficiently by aggregating neural point features near scene surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can be initialized via direct inference of a pre-trained deep network to produce a neural point cloud; this point cloud can be finetuned to surpass the visual quality of NeRF with 30× faster training time. Point-NeRF can be combined with other 3D re-construction methods and handles the errors and outliers in such methods via a novel pruning and growing mechanism.",
    full_text: "# Point-NeRF: Point-based Neural Radiance Fields\n\n## Abstract\n\nVolumetric neural rendering methods like NeRF [34] generate high-quality view synthesis results but are optimized per-scene leading to prohibitive reconstruction time. On the other hand, deep multi-view stereo methods can quickly reconstruct scene geometry via direct network inference. Point-NeRF combines the advantages of these two approaches by using neural 3D point clouds, with associated neural features, to model a radiance field. Point-NeRF can be rendered efficiently by aggregating neural point features near scene surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can be initialized via direct inference of a pre-trained deep network to produce a neural point cloud; this point cloud can be finetuned to surpass the visual quality of NeRF with 30× faster training time. Point-NeRF can be combined with other 3D re-construction methods and handles the errors and outliers in such methods via a novel pruning and growing mechanism.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nVolumetric neural rendering methods like NeRF [34] generate high-quality view synthesis results but are optimized per-scene leading to prohibitive reconstruction time.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Qiangeng Xu","Zexiang Xu","J. Philip","Sai Bi","Zhixin Shu","Kalyan Sunkavalli","U. Neumann"],
    arxiv_id: "2201.08845",
    publication_date: new Date("2022-01-21T00:00:00.000Z"),
    venue: "Computer Vision and Pattern Recognition",
    doi: "10.1109/CVPR52688.2022.00536",
    citation_count: 662,
  },
  {
    title: "Instant neural graphics primitives with a multiresolution hash encoding",
    abstract: "Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920×1080.",
    full_text: "# Instant neural graphics primitives with a multiresolution hash encoding\n\n## Abstract\n\nNeural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920×1080.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nNeural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["T. Müller","Alex Evans","Christoph Schied","A. Keller"],
    arxiv_id: "2201.05989",
    publication_date: new Date("2022-01-16T00:00:00.000Z"),
    venue: "ACM Transactions on Graphics",
    doi: "10.1145/3528223.3530127",
    citation_count: 4843,
  },
  {
    title: "Plenoxels: Radiance Fields without Neural Networks",
    abstract: "We introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels.",
    full_text: "# Plenoxels: Radiance Fields without Neural Networks\n\n## Abstract\n\nWe introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nWe introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Alex Yu","Sara Fridovich-Keil","Matthew Tancik","Qinhong Chen","B. Recht","Angjoo Kanazawa"],
    arxiv_id: "2112.05131",
    publication_date: new Date("2021-12-09T00:00:00.000Z"),
    venue: "Computer Vision and Pattern Recognition",
    doi: "10.1109/CVPR52688.2022.00542",
    citation_count: 1929,
  },
  {
    title: "Neural Fields in Visual Computing and Beyond",
    abstract: "Recent advances in machine learning have led to increased interest in solving visual computing problems using methods that employ coordinate‐based neural networks. These methods, which we call neural fields, parameterize physical properties of scenes or objects across space and time. They have seen widespread success in problems such as 3D shape and image synthesis, animation of human bodies, 3D reconstruction, and pose estimation. Rapid progress has led to numerous papers, but a consolidation of the discovered knowledge has not yet emerged. We provide context, mathematical grounding, and a review of over 250 papers in the literature on neural fields. In Part I, we focus on neural field techniques by identifying common components of neural field methods, including different conditioning, representation, forward map, architecture, and manipulation methods. In Part II, we focus on applications of neural fields to different problems in visual computing, and beyond (e.g., robotics, audio). Our review shows the breadth of topics already covered in visual computing, both historically and in current incarnations, and highlights the improved quality, flexibility, and capability brought by neural field methods. Finally, we present a companion website that acts as a living database that can be continually updated by the community.",
    full_text: "# Neural Fields in Visual Computing and Beyond\n\n## Abstract\n\nRecent advances in machine learning have led to increased interest in solving visual computing problems using methods that employ coordinate‐based neural networks. These methods, which we call neural fields, parameterize physical properties of scenes or objects across space and time. They have seen widespread success in problems such as 3D shape and image synthesis, animation of human bodies, 3D reconstruction, and pose estimation. Rapid progress has led to numerous papers, but a consolidation of the discovered knowledge has not yet emerged. We provide context, mathematical grounding, and a review of over 250 papers in the literature on neural fields. In Part I, we focus on neural field techniques by identifying common components of neural field methods, including different conditioning, representation, forward map, architecture, and manipulation methods. In Part II, we focus on applications of neural fields to different problems in visual computing, and beyond (e.g., robotics, audio). Our review shows the breadth of topics already covered in visual computing, both historically and in current incarnations, and highlights the improved quality, flexibility, and capability brought by neural field methods. Finally, we present a companion website that acts as a living database that can be continually updated by the community.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nRecent advances in machine learning have led to increased interest in solving visual computing problems using methods that employ coordinate‐based neural networks.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Yiheng Xie","Towaki Takikawa","Shunsuke Saito","O. Litany","Shiqin Yan","Numair Khan","Federico Tombari","James Tompkin","Vincent Sitzmann","Srinath Sridhar"],
    arxiv_id: "2111.11426",
    publication_date: new Date("2021-11-22T00:00:00.000Z"),
    venue: "Computer graphics forum (Print)",
    doi: "10.1111/cgf.14505",
    citation_count: 715,
  },
  {
    title: "Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction",
    abstract: "We present a super-fast convergence approach to reconstructing the per-scene radiance field from a set of images that capture the scene with known poses. This task, which is often applied to novel view synthesis, is recently revolution-ized by Neural Radiance Field (NeRF) for its state-of-the-art quality and fiexibility. However, NeRF and its variants require a lengthy training time ranging from hours to days for a single scene. In contrast, our approach achieves NeRF-comparable quality and converges rapidly from scratch in less than 15 minutes with a single GPU. We adopt a representation consisting of a density voxel grid for scene geometry and a feature voxel grid with a shallow network for complex view-dependent appearance. Modeling with explicit and discretized volume representations is not new, but we propose two simple yet non-trivial techniques that contribute to fast convergence speed and high-quality output. First, we introduce the post-activation interpolation on voxel density, which is capable of producing sharp surfaces in lower grid resolution. Second, direct voxel density optimization is prone to suboptimal geometry solutions, so we robustify the optimization process by imposing several priors. Finally, evaluation on five inward-facing benchmarks shows that our method matches, if not surpasses, NeRF's quality, yet it only takes about 15 minutes to train from scratch for a new scene. Code: https://github.com/sunset1995/DirectVoxGO.",
    full_text: "# Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction\n\n## Abstract\n\nWe present a super-fast convergence approach to reconstructing the per-scene radiance field from a set of images that capture the scene with known poses. This task, which is often applied to novel view synthesis, is recently revolution-ized by Neural Radiance Field (NeRF) for its state-of-the-art quality and fiexibility. However, NeRF and its variants require a lengthy training time ranging from hours to days for a single scene. In contrast, our approach achieves NeRF-comparable quality and converges rapidly from scratch in less than 15 minutes with a single GPU. We adopt a representation consisting of a density voxel grid for scene geometry and a feature voxel grid with a shallow network for complex view-dependent appearance. Modeling with explicit and discretized volume representations is not new, but we propose two simple yet non-trivial techniques that contribute to fast convergence speed and high-quality output. First, we introduce the post-activation interpolation on voxel density, which is capable of producing sharp surfaces in lower grid resolution. Second, direct voxel density optimization is prone to suboptimal geometry solutions, so we robustify the optimization process by imposing several priors. Finally, evaluation on five inward-facing benchmarks shows that our method matches, if not surpasses, NeRF's quality, yet it only takes about 15 minutes to train from scratch for a new scene. Code: https://github.com/sunset1995/DirectVoxGO.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nWe present a super-fast convergence approach to reconstructing the per-scene radiance field from a set of images that capture the scene with known poses.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Cheng Sun","Min Sun","Hwann-Tzong Chen"],
    arxiv_id: "2111.11215",
    publication_date: new Date("2021-11-22T00:00:00.000Z"),
    venue: "Computer Vision and Pattern Recognition",
    doi: "10.1109/CVPR52688.2022.00538",
    citation_count: 1229,
  },
  {
    title: "Advances in Neural Rendering",
    abstract: "Synthesizing photo‐realistic images and videos is at the heart of computer graphics and has been the focus of decades of research. Traditionally, synthetic images of a scene are generated using rendering algorithms such as rasterization or ray tracing, which take specifically defined representations of geometry and material properties as input. Collectively, these inputs define the actual scene and what is rendered, and are referred to as the scene representation (where a scene consists of one or more objects). Example scene representations are triangle meshes with accompanied textures (e.g., created by an artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g., from a CT scan), or implicit surface functions (e.g., truncated signed distance fields). The reconstruction of such a scene representation from observations using differentiable rendering losses is known as inverse graphics or inverse rendering. Neural rendering is closely related, and combines ideas from classical computer graphics and machine learning to create algorithms for synthesizing images from real‐world observations. Neural rendering is a leap forward towards the goal of synthesizing photo‐realistic image and video content. In recent years, we have seen immense progress in this field through hundreds of publications that show different ways to inject learnable components into the rendering pipeline. This state‐of‐the‐art report on advances in neural rendering focuses on methods that combine classical rendering principles with learned 3D scene representations, often now referred to as neural scene representations. A key advantage of these methods is that they are 3D‐consistent by design, enabling applications such as novel viewpoint synthesis of a captured scene. In addition to methods that handle static scenes, we cover neural scene representations for modeling non‐rigidly deforming objects and scene editing and composition. While most of these approaches are scene‐specific, we also discuss techniques that generalize across object classes and can be used for generative tasks. In addition to reviewing these state‐of‐the‐art methods, we provide an overview of fundamental concepts and definitions used in the current literature. We conclude with a discussion on open challenges and social implications.",
    full_text: "# Advances in Neural Rendering\n\n## Abstract\n\nSynthesizing photo‐realistic images and videos is at the heart of computer graphics and has been the focus of decades of research. Traditionally, synthetic images of a scene are generated using rendering algorithms such as rasterization or ray tracing, which take specifically defined representations of geometry and material properties as input. Collectively, these inputs define the actual scene and what is rendered, and are referred to as the scene representation (where a scene consists of one or more objects). Example scene representations are triangle meshes with accompanied textures (e.g., created by an artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g., from a CT scan), or implicit surface functions (e.g., truncated signed distance fields). The reconstruction of such a scene representation from observations using differentiable rendering losses is known as inverse graphics or inverse rendering. Neural rendering is closely related, and combines ideas from classical computer graphics and machine learning to create algorithms for synthesizing images from real‐world observations. Neural rendering is a leap forward towards the goal of synthesizing photo‐realistic image and video content. In recent years, we have seen immense progress in this field through hundreds of publications that show different ways to inject learnable components into the rendering pipeline. This state‐of‐the‐art report on advances in neural rendering focuses on methods that combine classical rendering principles with learned 3D scene representations, often now referred to as neural scene representations. A key advantage of these methods is that they are 3D‐consistent by design, enabling applications such as novel viewpoint synthesis of a captured scene. In addition to methods that handle static scenes, we cover neural scene representations for modeling non‐rigidly deforming objects and scene editing and composition. While most of these approaches are scene‐specific, we also discuss techniques that generalize across object classes and can be used for generative tasks. In addition to reviewing these state‐of‐the‐art methods, we provide an overview of fundamental concepts and definitions used in the current literature. We conclude with a discussion on open challenges and social implications.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nSynthesizing photo‐realistic images and videos is at the heart of computer graphics and has been the focus of decades of research.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["A. Tewari","O. Fried","Justus Thies","V. Sitzmann","S. Lombardi","Z. Xu","T. Simon","M. Nießner","E. Tretschk","L. Liu","B. Mildenhall","P. Srinivasan","R. Pandey","S. Orts-Escolano","S. Fanello","M. Guo","Gordon Wetzstein","J. Zhu","C. Theobalt","M. Agrawala","D. B. Goldman","M. Zollhöfer"],
    arxiv_id: "2111.05849",
    publication_date: new Date("2021-07-21T00:00:00.000Z"),
    venue: "SIGGRAPH Courses",
    doi: "10.1111/cgf.14507",
    citation_count: 500,
  },
  {
    title: "Point‐Based Neural Rendering with Per‐View Optimization",
    abstract: "There has recently been great interest in neural rendering methods. Some approaches use 3D geometry reconstructed with Multi‐View Stereo (MVS) but cannot recover from the errors of this process, while others directly learn a volumetric neural representation, but suffer from expensive training and inference. We introduce a general approach that is initialized with MVS, but allows further optimization of scene properties in the space of input views, including depth and reprojected features, resulting in improved novel‐view synthesis. A key element of our approach is our new differentiable point‐based pipeline, based on bi‐directional Elliptical Weighted Average splatting, a probabilistic depth test and effective camera selection. We use these elements together in our neural renderer, that outperforms all previous methods both in quality and speed in almost all scenes we tested. Our pipeline can be applied to multi‐view harmonization and stylization in addition to novel‐view synthesis.",
    full_text: "# Point‐Based Neural Rendering with Per‐View Optimization\n\n## Abstract\n\nThere has recently been great interest in neural rendering methods. Some approaches use 3D geometry reconstructed with Multi‐View Stereo (MVS) but cannot recover from the errors of this process, while others directly learn a volumetric neural representation, but suffer from expensive training and inference. We introduce a general approach that is initialized with MVS, but allows further optimization of scene properties in the space of input views, including depth and reprojected features, resulting in improved novel‐view synthesis. A key element of our approach is our new differentiable point‐based pipeline, based on bi‐directional Elliptical Weighted Average splatting, a probabilistic depth test and effective camera selection. We use these elements together in our neural renderer, that outperforms all previous methods both in quality and speed in almost all scenes we tested. Our pipeline can be applied to multi‐view harmonization and stylization in addition to novel‐view synthesis.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nThere has recently been great interest in neural rendering methods.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Georgios Kopanas","J. Philip","Thomas Leimkühler","G. Drettakis"],
    arxiv_id: "2109.02369",
    publication_date: new Date("2021-07-01T00:00:00.000Z"),
    venue: "Computer graphics forum (Print)",
    doi: "10.1111/cgf.14339",
    citation_count: 235,
  },
  {
    title: "Baking Neural Radiance Fields for Real-Time View Synthesis",
    abstract: "Neural volumetric representations such as Neural Radiance Fields (NeRF) have emerged as a compelling technique for learning to represent 3D scenes from images with the goal of rendering photorealistic images of the scene from unobserved viewpoints. However, NeRF’s computational requirements are prohibitive for real-time applications: rendering views from a trained NeRF requires querying a multilayer perceptron (MLP) hundreds of times per ray. We present a method to train a NeRF, then precompute and store (i.e. \"bake\") it as a novel representation called a Sparse Neural Radiance Grid (SNeRG) that enables real-time rendering on commodity hardware. To achieve this, we introduce 1) a reformulation of NeRF’s architecture, and 2) a sparse voxel grid representation with learned feature vectors. The resulting scene representation retains NeRF’s ability to render fine geometric details and view-dependent appearance, is compact (averaging less than 90 MB per scene), and can be rendered in real-time (higher than 30 frames per second on a laptop GPU). Actual screen captures are shown in our video.",
    full_text: "# Baking Neural Radiance Fields for Real-Time View Synthesis\n\n## Abstract\n\nNeural volumetric representations such as Neural Radiance Fields (NeRF) have emerged as a compelling technique for learning to represent 3D scenes from images with the goal of rendering photorealistic images of the scene from unobserved viewpoints. However, NeRF’s computational requirements are prohibitive for real-time applications: rendering views from a trained NeRF requires querying a multilayer perceptron (MLP) hundreds of times per ray. We present a method to train a NeRF, then precompute and store (i.e. \"bake\") it as a novel representation called a Sparse Neural Radiance Grid (SNeRG) that enables real-time rendering on commodity hardware. To achieve this, we introduce 1) a reformulation of NeRF’s architecture, and 2) a sparse voxel grid representation with learned feature vectors. The resulting scene representation retains NeRF’s ability to render fine geometric details and view-dependent appearance, is compact (averaging less than 90 MB per scene), and can be rendered in real-time (higher than 30 frames per second on a laptop GPU). Actual screen captures are shown in our video.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nNeural volumetric representations such as Neural Radiance Fields (NeRF) have emerged as a compelling technique for learning to represent 3D scenes from images with the goal of rendering photorealistic images of the scene from unobserved viewpoints.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Peter Hedman","Pratul P. Srinivasan","B. Mildenhall","J. Barron","P. Debevec"],
    arxiv_id: "2103.14645",
    publication_date: new Date("2021-03-26T00:00:00.000Z"),
    venue: "IEEE International Conference on Computer Vision",
    doi: "10.1109/ICCV48922.2021.00582",
    citation_count: 613,
  },
  {
    title: "KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs",
    abstract: "NeRF synthesizes novel views of a scene with unprecedented quality by fitting a neural radiance field to RGB images. However, NeRF requires querying a deep Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering times, even on modern GPUs. In this paper, we demonstrate that real-time rendering is possible by utilizing thousands of tiny MLPs instead of one single large MLP. In our setting, each individual MLP only needs to represent parts of the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining this divide-and-conquer strategy with further optimizations, rendering is accelerated by three orders of magnitude compared to the original NeRF model without incurring high storage costs. Further, using teacher-student distillation for training, we show that this speed-up can be achieved without sacrificing visual quality.",
    full_text: "# KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs\n\n## Abstract\n\nNeRF synthesizes novel views of a scene with unprecedented quality by fitting a neural radiance field to RGB images. However, NeRF requires querying a deep Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering times, even on modern GPUs. In this paper, we demonstrate that real-time rendering is possible by utilizing thousands of tiny MLPs instead of one single large MLP. In our setting, each individual MLP only needs to represent parts of the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining this divide-and-conquer strategy with further optimizations, rendering is accelerated by three orders of magnitude compared to the original NeRF model without incurring high storage costs. Further, using teacher-student distillation for training, we show that this speed-up can be achieved without sacrificing visual quality.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nNeRF synthesizes novel views of a scene with unprecedented quality by fitting a neural radiance field to RGB images.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Christian Reiser","Songyou Peng","Yiyi Liao","Andreas Geiger"],
    arxiv_id: "2103.13744",
    publication_date: new Date("2021-03-25T00:00:00.000Z"),
    venue: "IEEE International Conference on Computer Vision",
    doi: "10.1109/ICCV48922.2021.01407",
    citation_count: 901,
  },
  {
    title: "PlenOctrees for Real-time Rendering of Neural Radiance Fields",
    abstract: "We introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent effects. Our method can render 800×800 images at more than 150 FPS, which is over 3000 times faster than conventional NeRFs. We do so without sacrificing quality while preserving the ability of NeRFs to perform free-viewpoint rendering of scenes with arbitrary geometry and view-dependent effects. Real-time performance is achieved by pre-tabulating the NeRF into a PlenOctree. In order to preserve view-dependent effects such as specularities, we factorize the appearance via closed-form spherical basis functions. Specifically, we show that it is possible to train NeRFs to predict a spherical harmonic representation of radiance, removing the viewing direction as an input to the neural network. Furthermore, we show that PlenOctrees can be directly optimized to further minimize the reconstruction loss, which leads to equal or better quality compared to competing methods. Moreover, this octree optimization step can be used to reduce the training time, as we no longer need to wait for the NeRF training to converge fully. Our real-time neural rendering approach may potentially enable new applications such as 6-DOF industrial and product visualizations, as well as next generation AR/VR systems. PlenOctrees are amenable to in-browser rendering as well; please visit the project page for the interactive online demo, as well as video and code: https://alexyu.net/plenoctrees.",
    full_text: "# PlenOctrees for Real-time Rendering of Neural Radiance Fields\n\n## Abstract\n\nWe introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent effects. Our method can render 800×800 images at more than 150 FPS, which is over 3000 times faster than conventional NeRFs. We do so without sacrificing quality while preserving the ability of NeRFs to perform free-viewpoint rendering of scenes with arbitrary geometry and view-dependent effects. Real-time performance is achieved by pre-tabulating the NeRF into a PlenOctree. In order to preserve view-dependent effects such as specularities, we factorize the appearance via closed-form spherical basis functions. Specifically, we show that it is possible to train NeRFs to predict a spherical harmonic representation of radiance, removing the viewing direction as an input to the neural network. Furthermore, we show that PlenOctrees can be directly optimized to further minimize the reconstruction loss, which leads to equal or better quality compared to competing methods. Moreover, this octree optimization step can be used to reduce the training time, as we no longer need to wait for the NeRF training to converge fully. Our real-time neural rendering approach may potentially enable new applications such as 6-DOF industrial and product visualizations, as well as next generation AR/VR systems. PlenOctrees are amenable to in-browser rendering as well; please visit the project page for the interactive online demo, as well as video and code: https://alexyu.net/plenoctrees.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nWe introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent effects.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Alex Yu","Ruilong Li","Matthew Tancik","Hao Li","Ren Ng","Angjoo Kanazawa"],
    arxiv_id: "2103.14024",
    publication_date: new Date("2021-03-25T00:00:00.000Z"),
    venue: "IEEE International Conference on Computer Vision",
    doi: "10.1109/ICCV48922.2021.00570",
    citation_count: 1205,
  },
  {
    title: "Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields",
    abstract: "The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call \"mip-NeRF\" (à la \"mipmap\"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF’s ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22× faster.",
    full_text: "# Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields\n\n## Abstract\n\nThe rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call \"mip-NeRF\" (à la \"mipmap\"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF’s ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22× faster.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nThe rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["J. Barron","B. Mildenhall","Matthew Tancik","Peter Hedman","Ricardo Martin-Brualla","Pratul P. Srinivasan"],
    arxiv_id: "2103.13415",
    publication_date: new Date("2021-03-24T00:00:00.000Z"),
    venue: "IEEE International Conference on Computer Vision",
    doi: "10.1109/ICCV48922.2021.00580",
    citation_count: 2353,
  },
  {
    title: "FastNeRF: High-Fidelity Neural Rendering at 200FPS",
    abstract: "Recent work on Neural Radiance Fields (NeRF) showed how neural networks can be used to encode complex 3D environments that can be rendered photorealistically from novel viewpoints. Rendering these images is very computationally demanding and recent improvements are still a long way from enabling interactive rates, even on high-end hardware. Motivated by scenarios on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based system capable of rendering high fidelity photorealistic images at 200Hz on a high-end consumer GPU. The core of our method is a graphics-inspired factorization that allows for (i) compactly caching a deep radiance map at each position in space, (ii) efficiently querying that map using ray directions to estimate the pixel values in the rendered image. Extensive experiments show that the proposed method is 3000 times faster than the original NeRF algorithm and at least an order of magnitude faster than existing work on accelerating NeRF, while maintaining visual quality and extensibility.",
    full_text: "# FastNeRF: High-Fidelity Neural Rendering at 200FPS\n\n## Abstract\n\nRecent work on Neural Radiance Fields (NeRF) showed how neural networks can be used to encode complex 3D environments that can be rendered photorealistically from novel viewpoints. Rendering these images is very computationally demanding and recent improvements are still a long way from enabling interactive rates, even on high-end hardware. Motivated by scenarios on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based system capable of rendering high fidelity photorealistic images at 200Hz on a high-end consumer GPU. The core of our method is a graphics-inspired factorization that allows for (i) compactly caching a deep radiance map at each position in space, (ii) efficiently querying that map using ray directions to estimate the pixel values in the rendered image. Extensive experiments show that the proposed method is 3000 times faster than the original NeRF algorithm and at least an order of magnitude faster than existing work on accelerating NeRF, while maintaining visual quality and extensibility.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nRecent work on Neural Radiance Fields (NeRF) showed how neural networks can be used to encode complex 3D environments that can be rendered photorealistically from novel viewpoints.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Stephan J. Garbin","Marek Kowalski","Matthew Johnson","J. Shotton","Julien P. C. Valentin"],
    arxiv_id: "2103.10380",
    publication_date: new Date("2021-03-18T00:00:00.000Z"),
    venue: "IEEE International Conference on Computer Vision",
    doi: "10.1109/ICCV48922.2021.01408",
    citation_count: 720,
  },
  {
    title: "Mixture of volumetric primitives for efficient neural rendering",
    abstract: "Real-time rendering and animation of humans is a core function in games, movies, and telepresence applications. Existing methods have a number of drawbacks we aim to address with our work. Triangle meshes have difficulty modeling thin structures like hair, volumetric representations like Neural Volumes are too low-resolution given a reasonable memory budget, and high-resolution implicit representations like Neural Radiance Fields are too slow for use in real-time applications. We present Mixture of Volumetric Primitives (MVP), a representation for rendering dynamic 3D content that combines the completeness of volumetric representations with the efficiency of primitive-based rendering, e.g., point-based or mesh-based methods. Our approach achieves this by leveraging spatially shared computation with a convolutional architecture and by minimizing computation in empty regions of space with volumetric primitives that can move to cover only occupied regions. Our parameterization supports the integration of correspondence and tracking constraints, while being robust to areas where classical tracking fails, such as around thin or translucent structures and areas with large topological variability. MVP is a hybrid that generalizes both volumetric and primitive-based representations. Through a series of extensive experiments we demonstrate that it inherits the strengths of each, while avoiding many of their limitations. We also compare our approach to several state-of-the-art methods and demonstrate that MVP produces superior results in terms of quality and runtime performance.",
    full_text: "# Mixture of volumetric primitives for efficient neural rendering\n\n## Abstract\n\nReal-time rendering and animation of humans is a core function in games, movies, and telepresence applications. Existing methods have a number of drawbacks we aim to address with our work. Triangle meshes have difficulty modeling thin structures like hair, volumetric representations like Neural Volumes are too low-resolution given a reasonable memory budget, and high-resolution implicit representations like Neural Radiance Fields are too slow for use in real-time applications. We present Mixture of Volumetric Primitives (MVP), a representation for rendering dynamic 3D content that combines the completeness of volumetric representations with the efficiency of primitive-based rendering, e.g., point-based or mesh-based methods. Our approach achieves this by leveraging spatially shared computation with a convolutional architecture and by minimizing computation in empty regions of space with volumetric primitives that can move to cover only occupied regions. Our parameterization supports the integration of correspondence and tracking constraints, while being robust to areas where classical tracking fails, such as around thin or translucent structures and areas with large topological variability. MVP is a hybrid that generalizes both volumetric and primitive-based representations. Through a series of extensive experiments we demonstrate that it inherits the strengths of each, while avoiding many of their limitations. We also compare our approach to several state-of-the-art methods and demonstrate that MVP produces superior results in terms of quality and runtime performance.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nReal-time rendering and animation of humans is a core function in games, movies, and telepresence applications.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Stephen Lombardi","T. Simon","Gabriel Schwartz","Michael Zollhoefer","Yaser Sheikh","Jason M. Saragih"],
    arxiv_id: "2103.01954",
    publication_date: new Date("2021-03-02T00:00:00.000Z"),
    venue: "ACM Transactions on Graphics",
    doi: "10.1145/3476576.3476608",
    citation_count: 326,
  },
  {
    title: "Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes",
    abstract: "Neural signed distance functions (SDFs) are emerging as an effective representation for 3D shapes. State-of-the-art methods typically encode the SDF with a large, fixed-size neural network to approximate complex shapes with implicit surfaces. Rendering with these large networks is, however, computationally expensive since it requires many forward passes through the network for every pixel, making these representations impractical for real-time graphics. We introduce an efficient neural representation that, for the first time, enables real-time rendering of high-fidelity neural SDFs, while achieving state-of-the-art geometry reconstruction quality. We represent implicit surfaces using an octree-based feature volume which adaptively fits shapes with multiple discrete levels of detail (LODs), and enables continuous LOD with SDF interpolation. We further develop an efficient algorithm to directly render our novel neural SDF representation in real-time by querying only the necessary LODs with sparse octree traversal. We show that our representation is 2–3 orders of magnitude more efficient in terms of rendering speed compared to previous works. Furthermore, it produces state-of-the-art reconstruction quality for complex shapes under both 3D geometric and 2D image-space metrics.",
    full_text: "# Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes\n\n## Abstract\n\nNeural signed distance functions (SDFs) are emerging as an effective representation for 3D shapes. State-of-the-art methods typically encode the SDF with a large, fixed-size neural network to approximate complex shapes with implicit surfaces. Rendering with these large networks is, however, computationally expensive since it requires many forward passes through the network for every pixel, making these representations impractical for real-time graphics. We introduce an efficient neural representation that, for the first time, enables real-time rendering of high-fidelity neural SDFs, while achieving state-of-the-art geometry reconstruction quality. We represent implicit surfaces using an octree-based feature volume which adaptively fits shapes with multiple discrete levels of detail (LODs), and enables continuous LOD with SDF interpolation. We further develop an efficient algorithm to directly render our novel neural SDF representation in real-time by querying only the necessary LODs with sparse octree traversal. We show that our representation is 2–3 orders of magnitude more efficient in terms of rendering speed compared to previous works. Furthermore, it produces state-of-the-art reconstruction quality for complex shapes under both 3D geometric and 2D image-space metrics.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nNeural signed distance functions (SDFs) are emerging as an effective representation for 3D shapes.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Towaki Takikawa","Joey Litalien","K. Yin","Karsten Kreis","Charles T. Loop","D. Nowrouzezahrai","Alec Jacobson","M. McGuire","S. Fidler"],
    arxiv_id: "2101.10994",
    publication_date: new Date("2021-01-26T00:00:00.000Z"),
    venue: "Computer Vision and Pattern Recognition",
    doi: "10.1109/CVPR46437.2021.01120",
    citation_count: 573,
  },
  {
    title: "Pulsar: Efficient Sphere-based Neural Rendering",
    abstract: "We propose Pulsar, an efficient sphere-based differentiable rendering module that is orders of magnitude faster than competing techniques, modular, and easy-to-use due to its tight integration with PyTorch. Differentiable rendering is the foundation for modern neural rendering approaches, since it enables end-to-end training of 3D scene representations from image observations. However, gradient-based optimization of neural mesh, voxel, or function representations suffers from multiple challenges, i.e., topological in-consistencies, high memory footprints, or slow rendering speeds. To alleviate these problems, Pulsar employs: 1) a sphere-based scene representation, 2) a modular, efficient differentiable projection operation, and 3) (optional) neural shading. Pulsar executes orders of magnitude faster than existing techniques and allows real-time rendering and optimization of representations with millions of spheres. Using spheres for the scene representation, unprecedented speed is obtained while avoiding topology problems. Pulsar is fully differentiable and thus enables a plethora of applications, ranging from 3D reconstruction to neural rendering.",
    full_text: "# Pulsar: Efficient Sphere-based Neural Rendering\n\n## Abstract\n\nWe propose Pulsar, an efficient sphere-based differentiable rendering module that is orders of magnitude faster than competing techniques, modular, and easy-to-use due to its tight integration with PyTorch. Differentiable rendering is the foundation for modern neural rendering approaches, since it enables end-to-end training of 3D scene representations from image observations. However, gradient-based optimization of neural mesh, voxel, or function representations suffers from multiple challenges, i.e., topological in-consistencies, high memory footprints, or slow rendering speeds. To alleviate these problems, Pulsar employs: 1) a sphere-based scene representation, 2) a modular, efficient differentiable projection operation, and 3) (optional) neural shading. Pulsar executes orders of magnitude faster than existing techniques and allows real-time rendering and optimization of representations with millions of spheres. Using spheres for the scene representation, unprecedented speed is obtained while avoiding topology problems. Pulsar is fully differentiable and thus enables a plethora of applications, ranging from 3D reconstruction to neural rendering.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nWe propose Pulsar, an efficient sphere-based differentiable rendering module that is orders of magnitude faster than competing techniques, modular, and easy-to-use due to its tight integration with PyTorch.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Christoph Lassner","M. Zollhöfer"],
    arxiv_id: null,
    publication_date: new Date("2021-06-01T00:00:00.000Z"),
    venue: "Computer Vision and Pattern Recognition",
    doi: "10.1109/CVPR46437.2021.00149",
    citation_count: 202,
  },
  {
    title: "Free View Synthesis",
    abstract: "We present a method for novel view synthesis from input images that are freely distributed around a scene. Our method does not rely on a regular arrangement of input views, can synthesize images for free camera movement through the scene, and works for general scenes with unconstrained geometric layouts. We calibrate the input images via SfM and erect a coarse geometric scaffold via MVS. This scaffold is used to create a proxy depth map for a novel view of the scene. Based on this depth map, a recurrent encoder-decoder network processes reprojected features from nearby views and synthesizes the new view. Our network does not need to be optimized for a given scene. After training on a dataset, it works in previously unseen environments with no fine-tuning or per-scene optimization. We evaluate the presented approach on challenging real-world datasets, including Tanks and Temples, where we demonstrate successful view synthesis for the first time and substantially outperform prior and concurrent work.",
    full_text: "# Free View Synthesis\n\n## Abstract\n\nWe present a method for novel view synthesis from input images that are freely distributed around a scene. Our method does not rely on a regular arrangement of input views, can synthesize images for free camera movement through the scene, and works for general scenes with unconstrained geometric layouts. We calibrate the input images via SfM and erect a coarse geometric scaffold via MVS. This scaffold is used to create a proxy depth map for a novel view of the scene. Based on this depth map, a recurrent encoder-decoder network processes reprojected features from nearby views and synthesizes the new view. Our network does not need to be optimized for a given scene. After training on a dataset, it works in previously unseen environments with no fine-tuning or per-scene optimization. We evaluate the presented approach on challenging real-world datasets, including Tanks and Temples, where we demonstrate successful view synthesis for the first time and substantially outperform prior and concurrent work.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nWe present a method for novel view synthesis from input images that are freely distributed around a scene.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Gernot Riegler","V. Koltun"],
    arxiv_id: "2008.05511",
    publication_date: new Date("2020-08-12T00:00:00.000Z"),
    venue: "European Conference on Computer Vision",
    doi: "10.1007/978-3-030-58529-7_37",
    citation_count: 376,
  },
  {
    title: "SynSin: End-to-End View Synthesis From a Single Image",
    abstract: "View synthesis allows for the generation of new views of a scene given one or more images. This is challenging; it requires comprehensively understanding the 3D scene from images. As a result, current methods typically use multiple images, train on ground-truth depth, or are limited to synthetic data. We propose a novel end-to-end model for this task using a single image at test time; it is trained on real images without any ground-truth 3D information. To this end, we introduce a novel differentiable point cloud renderer that is used to transform a latent 3D point cloud of features into the target view. The projected features are decoded by our refinement network to inpaint missing regions and generate a realistic output image. The 3D component inside of our generative model allows for interpretable manipulation of the latent feature space at test time, e.g. we can animate trajectories from a single image. Additionally, we can generate high resolution images and generalise to other input resolutions. We outperform baselines and prior work on the Matterport, Replica, and RealEstate10K datasets.",
    full_text: "# SynSin: End-to-End View Synthesis From a Single Image\n\n## Abstract\n\nView synthesis allows for the generation of new views of a scene given one or more images. This is challenging; it requires comprehensively understanding the 3D scene from images. As a result, current methods typically use multiple images, train on ground-truth depth, or are limited to synthetic data. We propose a novel end-to-end model for this task using a single image at test time; it is trained on real images without any ground-truth 3D information. To this end, we introduce a novel differentiable point cloud renderer that is used to transform a latent 3D point cloud of features into the target view. The projected features are decoded by our refinement network to inpaint missing regions and generate a realistic output image. The 3D component inside of our generative model allows for interpretable manipulation of the latent feature space at test time, e.g. we can animate trajectories from a single image. Additionally, we can generate high resolution images and generalise to other input resolutions. We outperform baselines and prior work on the Matterport, Replica, and RealEstate10K datasets.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nView synthesis allows for the generation of new views of a scene given one or more images.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Olivia Wiles","Georgia Gkioxari","R. Szeliski","Justin Johnson"],
    arxiv_id: "1912.08804",
    publication_date: new Date("2019-12-18T00:00:00.000Z"),
    venue: "Computer Vision and Pattern Recognition",
    doi: "10.1109/CVPR42600.2020.00749",
    citation_count: 510,
  },
  {
    title: "Neural Point-Based Graphics",
    abstract: "We present a new point-based approach for modeling the appearance of real scenes. The approach uses a raw point cloud as the geometric representation of a scene, and augments each point with a learnable neural descriptor that encodes local geometry and appearance. A deep rendering network is learned in parallel with the descriptors, so that new views of the scene can be obtained by passing the rasterizations of a point cloud from new viewpoints through this network. The input rasterizations use the learned descriptors as point pseudo-colors. We show that the proposed approach can be used for modeling complex scenes and obtaining their photorealistic views, while avoiding explicit surface estimation and meshing. In particular, compelling results are obtained for scene scanned using hand-held commodity RGB-D sensors as well as standard RGB cameras even in the presence of objects that are challenging for standard mesh-based modeling.",
    full_text: "# Neural Point-Based Graphics\n\n## Abstract\n\nWe present a new point-based approach for modeling the appearance of real scenes. The approach uses a raw point cloud as the geometric representation of a scene, and augments each point with a learnable neural descriptor that encodes local geometry and appearance. A deep rendering network is learned in parallel with the descriptors, so that new views of the scene can be obtained by passing the rasterizations of a point cloud from new viewpoints through this network. The input rasterizations use the learned descriptors as point pseudo-colors. We show that the proposed approach can be used for modeling complex scenes and obtaining their photorealistic views, while avoiding explicit surface estimation and meshing. In particular, compelling results are obtained for scene scanned using hand-held commodity RGB-D sensors as well as standard RGB cameras even in the presence of objects that are challenging for standard mesh-based modeling.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nWe present a new point-based approach for modeling the appearance of real scenes.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Kara-Ali Aliev","Dmitry Ulyanov","V. Lempitsky"],
    arxiv_id: "1906.08240",
    publication_date: new Date("2019-06-19T00:00:00.000Z"),
    venue: "European Conference on Computer Vision",
    doi: "10.1007/978-3-030-58542-6_42",
    citation_count: 464,
  },
  {
    title: "Differentiable surface splatting for point-based geometry processing",
    abstract: "We propose Differentiable Surface Splatting (DSS), a high-fidelity differentiable renderer for point clouds. Gradients for point locations and normals are carefully designed to handle discontinuities of the rendering function. Regularization terms are introduced to ensure uniform distribution of the points on the underlying surface. We demonstrate applications of DSS to inverse rendering for geometry synthesis and denoising, where large scale topological changes, as well as small scale detail modifications, are accurately and robustly handled without requiring explicit connectivity, outperforming state-of-the-art techniques. The data and code are at https://github.com/yifita/DSS.",
    full_text: "# Differentiable surface splatting for point-based geometry processing\n\n## Abstract\n\nWe propose Differentiable Surface Splatting (DSS), a high-fidelity differentiable renderer for point clouds. Gradients for point locations and normals are carefully designed to handle discontinuities of the rendering function. Regularization terms are introduced to ensure uniform distribution of the points on the underlying surface. We demonstrate applications of DSS to inverse rendering for geometry synthesis and denoising, where large scale topological changes, as well as small scale detail modifications, are accurately and robustly handled without requiring explicit connectivity, outperforming state-of-the-art techniques. The data and code are at https://github.com/yifita/DSS.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nWe propose Differentiable Surface Splatting (DSS), a high-fidelity differentiable renderer for point clouds.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Yifan Wang","Felice Serena","Shihao Wu","Cengiz Öztireli","O. Sorkine-Hornung"],
    arxiv_id: "1906.04173",
    publication_date: new Date("2019-06-10T00:00:00.000Z"),
    venue: "ACM Transactions on Graphics",
    doi: "10.1145/3355089.3356513",
    citation_count: 344,
  },
  {
    title: "Deferred neural rendering",
    abstract: "The modern computer graphics pipeline can synthesize images at remarkable visual quality; however, it requires well-defined, high-quality 3D content as input. In this work, we explore the use of imperfect 3D content, for instance, obtained from photo-metric reconstructions with noisy and incomplete surface geometry, while still aiming to produce photo-realistic (re-)renderings. To address this challenging problem, we introduce Deferred Neural Rendering, a new paradigm for image synthesis that combines the traditional graphics pipeline with learnable components. Specifically, we propose Neural Textures, which are learned feature maps that are trained as part of the scene capture process. Similar to traditional textures, neural textures are stored as maps on top of 3D mesh proxies; however, the high-dimensional feature maps contain significantly more information, which can be interpreted by our new deferred neural rendering pipeline. Both neural textures and deferred neural renderer are trained end-to-end, enabling us to synthesize photo-realistic images even when the original 3D content was imperfect. In contrast to traditional, black-box 2D generative neural networks, our 3D representation gives us explicit control over the generated output, and allows for a wide range of application domains. For instance, we can synthesize temporally-consistent video re-renderings of recorded 3D scenes as our representation is inherently embedded in 3D space. This way, neural textures can be utilized to coherently re-render or manipulate existing video content in both static and dynamic environments at real-time rates. We show the effectiveness of our approach in several experiments on novel view synthesis, scene editing, and facial reenactment, and compare to state-of-the-art approaches that leverage the standard graphics pipeline as well as conventional generative neural networks.",
    full_text: "# Deferred neural rendering\n\n## Abstract\n\nThe modern computer graphics pipeline can synthesize images at remarkable visual quality; however, it requires well-defined, high-quality 3D content as input. In this work, we explore the use of imperfect 3D content, for instance, obtained from photo-metric reconstructions with noisy and incomplete surface geometry, while still aiming to produce photo-realistic (re-)renderings. To address this challenging problem, we introduce Deferred Neural Rendering, a new paradigm for image synthesis that combines the traditional graphics pipeline with learnable components. Specifically, we propose Neural Textures, which are learned feature maps that are trained as part of the scene capture process. Similar to traditional textures, neural textures are stored as maps on top of 3D mesh proxies; however, the high-dimensional feature maps contain significantly more information, which can be interpreted by our new deferred neural rendering pipeline. Both neural textures and deferred neural renderer are trained end-to-end, enabling us to synthesize photo-realistic images even when the original 3D content was imperfect. In contrast to traditional, black-box 2D generative neural networks, our 3D representation gives us explicit control over the generated output, and allows for a wide range of application domains. For instance, we can synthesize temporally-consistent video re-renderings of recorded 3D scenes as our representation is inherently embedded in 3D space. This way, neural textures can be utilized to coherently re-render or manipulate existing video content in both static and dynamic environments at real-time rates. We show the effectiveness of our approach in several experiments on novel view synthesis, scene editing, and facial reenactment, and compare to state-of-the-art approaches that leverage the standard graphics pipeline as well as conventional generative neural networks.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nThe modern computer graphics pipeline can synthesize images at remarkable visual quality; however, it requires well-defined, high-quality 3D content as input.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Justus Thies","M. Zollhöfer","M. Nießner"],
    arxiv_id: "1904.12356",
    publication_date: new Date("2019-04-28T00:00:00.000Z"),
    venue: "ACM Transactions on Graphics",
    doi: "10.1145/3306346.3323035",
    citation_count: 734,
  },
  {
    title: "DeepVoxels: Learning Persistent 3D Feature Embeddings",
    abstract: "In this work, we address the lack of 3D understanding of generative neural networks by introducing a persistent 3D feature embedding for view synthesis. To this end, we propose DeepVoxels, a learned representation that encodes the view-dependent appearance of a 3D scene without having to explicitly model its geometry. At its core, our approach is based on a Cartesian 3D grid of persistent embedded features that learn to make use of the underlying 3D scene structure. Our approach combines insights from 3D geometric computer vision with recent advances in learning image-to-image mappings based on adversarial loss functions. DeepVoxels is supervised, without requiring a 3D reconstruction of the scene, using a 2D re-rendering loss and enforces perspective and multi-view geometry in a principled manner. We apply our persistent 3D scene representation to the problem of novel view synthesis demonstrating high-quality results for a variety of challenging scenes.",
    full_text: "# DeepVoxels: Learning Persistent 3D Feature Embeddings\n\n## Abstract\n\nIn this work, we address the lack of 3D understanding of generative neural networks by introducing a persistent 3D feature embedding for view synthesis. To this end, we propose DeepVoxels, a learned representation that encodes the view-dependent appearance of a 3D scene without having to explicitly model its geometry. At its core, our approach is based on a Cartesian 3D grid of persistent embedded features that learn to make use of the underlying 3D scene structure. Our approach combines insights from 3D geometric computer vision with recent advances in learning image-to-image mappings based on adversarial loss functions. DeepVoxels is supervised, without requiring a 3D reconstruction of the scene, using a 2D re-rendering loss and enforces perspective and multi-view geometry in a principled manner. We apply our persistent 3D scene representation to the problem of novel view synthesis demonstrating high-quality results for a variety of challenging scenes.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nIn this work, we address the lack of 3D understanding of generative neural networks by introducing a persistent 3D feature embedding for view synthesis.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["V. Sitzmann","Justus Thies","Felix Heide","M. Nießner","Gordon Wetzstein","M. Zollhöfer"],
    arxiv_id: "1812.01024",
    publication_date: new Date("2018-12-03T00:00:00.000Z"),
    venue: "Computer Vision and Pattern Recognition",
    doi: "10.1109/CVPR.2019.00254",
    citation_count: 705,
  },
  {
    title: "Escaping Plato’s Cave: 3D Shape From Adversarial Rendering",
    abstract: "We introduce PlatonicGAN to discover the 3D structure of an object class from an unstructured collection of 2D images, i.e., where no relation between photos is known, except that they are showing instances of the same category. The key idea is to train a deep neural network to generate 3D shapes which, when rendered to images, are indistinguishable from ground truth images (for a discriminator) under various camera poses. Discriminating 2D images instead of 3D shapes allows tapping into unstructured 2D photo collections instead of relying on curated (e.g., aligned, annotated, etc.) 3D data sets. To establish constraints between 2D image observation and their 3D interpretation, we suggest a family of rendering layers that are effectively differentiable. This family includes visual hull, absorption-only (akin to x-ray), and emission-absorption. We can successfully reconstruct 3D shapes from unstructured 2D images and extensively evaluate PlatonicGAN on a range of synthetic and real data sets achieving consistent improvements over baseline methods. We further show that PlatonicGAN can be combined with 3D supervision to improve on and in some cases even surpass the quality of 3D-supervised methods.",
    full_text: "# Escaping Plato’s Cave: 3D Shape From Adversarial Rendering\n\n## Abstract\n\nWe introduce PlatonicGAN to discover the 3D structure of an object class from an unstructured collection of 2D images, i.e., where no relation between photos is known, except that they are showing instances of the same category. The key idea is to train a deep neural network to generate 3D shapes which, when rendered to images, are indistinguishable from ground truth images (for a discriminator) under various camera poses. Discriminating 2D images instead of 3D shapes allows tapping into unstructured 2D photo collections instead of relying on curated (e.g., aligned, annotated, etc.) 3D data sets. To establish constraints between 2D image observation and their 3D interpretation, we suggest a family of rendering layers that are effectively differentiable. This family includes visual hull, absorption-only (akin to x-ray), and emission-absorption. We can successfully reconstruct 3D shapes from unstructured 2D images and extensively evaluate PlatonicGAN on a range of synthetic and real data sets achieving consistent improvements over baseline methods. We further show that PlatonicGAN can be combined with 3D supervision to improve on and in some cases even surpass the quality of 3D-supervised methods.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nWe introduce PlatonicGAN to discover the 3D structure of an object class from an unstructured collection of 2D images, i.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["P. Henzler","N. Mitra","Tobias Ritschel"],
    arxiv_id: "1811.11606",
    publication_date: new Date("2018-11-28T00:00:00.000Z"),
    venue: "IEEE International Conference on Computer Vision",
    doi: "10.1109/ICCV.2019.01008",
    citation_count: 244,
  },
  {
    title: "Deep blending for free-viewpoint image-based rendering",
    abstract: "Free-viewpoint image-based rendering (IBR) is a standing challenge. IBR methods combine warped versions of input photos to synthesize a novel view. The image quality of this combination is directly affected by geometric inaccuracies of multi-view stereo (MVS) reconstruction and by view- and image-dependent effects that produce artifacts when contributions from different input views are blended. We present a new deep learning approach to blending for IBR, in which we use held-out real image data to learn blending weights to combine input photo contributions. Our Deep Blending method requires us to address several challenges to achieve our goal of interactive free-viewpoint IBR navigation. We first need to provide sufficiently accurate geometry so the Convolutional Neural Network (CNN) can succeed in finding correct blending weights. We do this by combining two different MVS reconstructions with complementary accuracy vs. completeness tradeoffs. To tightly integrate learning in an interactive IBR system, we need to adapt our rendering algorithm to produce a fixed number of input layers that can then be blended by the CNN. We generate training data with a variety of captured scenes, using each input photo as ground truth in a held-out approach. We also design the network architecture and the training loss to provide high quality novel view synthesis, while reducing temporal flickering artifacts. Our results demonstrate free-viewpoint IBR in a wide variety of scenes, clearly surpassing previous methods in visual quality, especially when moving far from the input cameras.",
    full_text: "# Deep blending for free-viewpoint image-based rendering\n\n## Abstract\n\nFree-viewpoint image-based rendering (IBR) is a standing challenge. IBR methods combine warped versions of input photos to synthesize a novel view. The image quality of this combination is directly affected by geometric inaccuracies of multi-view stereo (MVS) reconstruction and by view- and image-dependent effects that produce artifacts when contributions from different input views are blended. We present a new deep learning approach to blending for IBR, in which we use held-out real image data to learn blending weights to combine input photo contributions. Our Deep Blending method requires us to address several challenges to achieve our goal of interactive free-viewpoint IBR navigation. We first need to provide sufficiently accurate geometry so the Convolutional Neural Network (CNN) can succeed in finding correct blending weights. We do this by combining two different MVS reconstructions with complementary accuracy vs. completeness tradeoffs. To tightly integrate learning in an interactive IBR system, we need to adapt our rendering algorithm to produce a fixed number of input layers that can then be blended by the CNN. We generate training data with a variety of captured scenes, using each input photo as ground truth in a held-out approach. We also design the network architecture and the training loss to provide high quality novel view synthesis, while reducing temporal flickering artifacts. Our results demonstrate free-viewpoint IBR in a wide variety of scenes, clearly surpassing previous methods in visual quality, especially when moving far from the input cameras.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nFree-viewpoint image-based rendering (IBR) is a standing challenge.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Peter Hedman","J. Philip","True Price","Jan-Michael Frahm","G. Drettakis","G. Brostow"],
    arxiv_id: null,
    publication_date: new Date("2018-12-04T00:00:00.000Z"),
    venue: "ACM Transactions on Graphics",
    doi: "10.1145/3272127.3275084",
    citation_count: 672,
  },
  {
    title: "Comments on \"Compression of 3D Point Clouds Using a Region-Adaptive Hierarchical Transform\"",
    abstract: "The recently introduced coder based on region-adaptive hierarchical transform (RAHT) for the compression of point clouds attributes, was shown to have a performance competitive with the state-of-the-art, while being much less complex. In the paper \"Compression of 3D Point Clouds Using a Region-Adaptive Hierarchical Transform\", top performance was achieved using arithmetic coding (AC), while adaptive run-length Golomb-Rice (RLGR) coding was presented as a lower-performance lower-complexity alternative. However, we have found that by reordering the RAHT coefficients we can largely increase the runs of zeros and significantly increase the performance of the RLGR-based RAHT coder. As a result, the new coder, using ordered coefficients, was shown to outperform all other coders, including AC-based RAHT, at an even lower computational cost. We present new results and plots that should enhance those in the work of Queiroz and Chou to include the new results for RLGR-RAHT. We risk to say, based on the results herein, that RLGR-RAHT with sorted coefficients is the new state-of-the-art in point cloud compression.",
    full_text: "# Comments on \"Compression of 3D Point Clouds Using a Region-Adaptive Hierarchical Transform\"\n\n## Abstract\n\nThe recently introduced coder based on region-adaptive hierarchical transform (RAHT) for the compression of point clouds attributes, was shown to have a performance competitive with the state-of-the-art, while being much less complex. In the paper \"Compression of 3D Point Clouds Using a Region-Adaptive Hierarchical Transform\", top performance was achieved using arithmetic coding (AC), while adaptive run-length Golomb-Rice (RLGR) coding was presented as a lower-performance lower-complexity alternative. However, we have found that by reordering the RAHT coefficients we can largely increase the runs of zeros and significantly increase the performance of the RLGR-based RAHT coder. As a result, the new coder, using ordered coefficients, was shown to outperform all other coders, including AC-based RAHT, at an even lower computational cost. We present new results and plots that should enhance those in the work of Queiroz and Chou to include the new results for RLGR-RAHT. We risk to say, based on the results herein, that RLGR-RAHT with sorted coefficients is the new state-of-the-art in point cloud compression.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nThe recently introduced coder based on region-adaptive hierarchical transform (RAHT) for the compression of point clouds attributes, was shown to have a performance competitive with the state-of-the-art, while being much less complex.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Gustavo P. Sandri","Ricado L. de Queiroz","P. Chou"],
    arxiv_id: "1805.09146",
    publication_date: new Date("2018-05-23T00:00:00.000Z"),
    venue: "Unknown",
    doi: null,
    citation_count: 90,
  },
  {
    title: "Tanks and temples",
    abstract: "We present a benchmark for image-based 3D reconstruction. The benchmark sequences were acquired outside the lab, in realistic conditions. Ground-truth data was captured using an industrial laser scanner. The benchmark includes both outdoor scenes and indoor environments. High-resolution video sequences are provided as input, supporting the development of novel pipelines that take advantage of video input to increase reconstruction fidelity. We report the performance of many image-based 3D reconstruction pipelines on the new benchmark. The results point to exciting challenges and opportunities for future work.",
    full_text: "# Tanks and temples\n\n## Abstract\n\nWe present a benchmark for image-based 3D reconstruction. The benchmark sequences were acquired outside the lab, in realistic conditions. Ground-truth data was captured using an industrial laser scanner. The benchmark includes both outdoor scenes and indoor environments. High-resolution video sequences are provided as input, supporting the development of novel pipelines that take advantage of video input to increase reconstruction fidelity. We report the performance of many image-based 3D reconstruction pipelines on the new benchmark. The results point to exciting challenges and opportunities for future work.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nWe present a benchmark for image-based 3D reconstruction.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["A. Knapitsch","Jaesik Park","Qian-Yi Zhou","V. Koltun"],
    arxiv_id: null,
    publication_date: new Date("2017-07-20T00:00:00.000Z"),
    venue: "ACM Transactions on Graphics",
    doi: "10.1145/3072959.3073599",
    citation_count: 682,
  },
  {
    title: "Soft 3D reconstruction for view synthesis",
    abstract: "Abstract not available",
    full_text: "# Soft 3D reconstruction for view synthesis\n\n## Abstract\n\nAbstract not available\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nAbstract not available.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Eric Penner","Li Zhang"],
    arxiv_id: null,
    publication_date: new Date("2017-11-20T00:00:00.000Z"),
    venue: "ACM Transactions on Graphics",
    doi: "10.1145/3130800.3130855",
    citation_count: 400,
  },
  {
    title: "Software Rasterization of 2 Billion Points in Real Time",
    abstract: "The accelerated collection of detailed real-world 3D data in the form of ever-larger point clouds is sparking a demand for novel visualization techniques that are capable of rendering billions of point primitives in real-time. We propose a software rasterization pipeline for point clouds that is capable of rendering up to two billion points in real-time (60 FPS) on commodity hardware. Improvements over the state of the art are achieved by batching points, enabling a number of batch-level optimizations before rasterizing them within the same rendering pass. These optimizations include frustum culling, level-of-detail (LOD) rendering, and choosing the appropriate coordinate precision for a given batch of points directly within a compute workgroup. Adaptive coordinate precision, in conjunction with visibility buffers, reduces the required data for the majority of points to just four bytes, making our approach several times faster than the bandwidth-limited state of the art. Furthermore, support for LOD rendering makes our software rasterization approach suitable for rendering arbitrarily large point clouds, and to meet the elevated performance demands of virtual reality applications.",
    full_text: "# Software Rasterization of 2 Billion Points in Real Time\n\n## Abstract\n\nThe accelerated collection of detailed real-world 3D data in the form of ever-larger point clouds is sparking a demand for novel visualization techniques that are capable of rendering billions of point primitives in real-time. We propose a software rasterization pipeline for point clouds that is capable of rendering up to two billion points in real-time (60 FPS) on commodity hardware. Improvements over the state of the art are achieved by batching points, enabling a number of batch-level optimizations before rasterizing them within the same rendering pass. These optimizations include frustum culling, level-of-detail (LOD) rendering, and choosing the appropriate coordinate precision for a given batch of points directly within a compute workgroup. Adaptive coordinate precision, in conjunction with visibility buffers, reduces the required data for the majority of points to just four bytes, making our approach several times faster than the bandwidth-limited state of the art. Furthermore, support for LOD rendering makes our software rasterization approach suitable for rendering arbitrarily large point clouds, and to meet the elevated performance demands of virtual reality applications.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nThe accelerated collection of detailed real-world 3D data in the form of ever-larger point clouds is sparking a demand for novel visualization techniques that are capable of rendering billions of point primitives in real-time.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Markus Schütz","Bernhard Kerbl","M. Wimmer"],
    arxiv_id: "2204.01287",
    publication_date: new Date("2022-04-04T00:00:00.000Z"),
    venue: "Proceedings of the ACM on Computer Graphics and Interactive Techniques",
    doi: "10.1145/3543863",
    citation_count: 42,
  },
  {
    title: "Online Submission ID: 0184 Photo Tourism: Exploring Photo Collections in 3D",
    abstract: "Abstract not available",
    full_text: "# Online Submission ID: 0184 Photo Tourism: Exploring Photo Collections in 3D\n\n## Abstract\n\nAbstract not available\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nAbstract not available.\n\n## References\n\n[References would be extracted from PDF]",
    authors: [],
    arxiv_id: null,
    publication_date: new Date("1900-01-01T06:00:00.000Z"),
    venue: "Unknown",
    doi: null,
    citation_count: 3753,
  },
  {
    title: "Eurographics Symposium on Point-based Graphics (2005) High-quality Surface Splatting on Today's Gpus",
    abstract: "Abstract not available",
    full_text: "# Eurographics Symposium on Point-based Graphics (2005) High-quality Surface Splatting on Today's Gpus\n\n## Abstract\n\nAbstract not available\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nAbstract not available.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["M. Botsch","A. Sorkine-Hornung","Matthias Zwicker","L. Kobbelt"],
    arxiv_id: null,
    publication_date: new Date("1900-01-01T06:00:00.000Z"),
    venue: "Unknown",
    doi: null,
    citation_count: 237,
  },
  {
    title: "Representing Scenes as Neural Radiance Fields for View Synthesis",
    abstract: "Abstract not available",
    full_text: "# Representing Scenes as Neural Radiance Fields for View Synthesis\n\n## Abstract\n\nAbstract not available\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nAbstract not available.\n\n## References\n\n[References would be extracted from PDF]",
    authors: [],
    arxiv_id: null,
    publication_date: new Date("1900-01-01T06:00:00.000Z"),
    venue: "Unknown",
    doi: null,
    citation_count: 4951,
  },
  {
    title: "Scalable neural indoor scene rendering",
    abstract: "We propose a scalable neural scene reconstruction and rendering method to support distributed training and interactive rendering of large indoor scenes. Our representation is based on tiles. Tile appearances are trained in parallel through a background sampling strategy that augments each tile with distant scene information via a proxy global mesh. Each tile has two low-capacity MLPs: one for view-independent appearance (diffuse color and shading) and one for view-dependent appearance (specular highlights, reflections). We leverage the phenomena that complex view-dependent scene reflections can be attributed to virtual lights underneath surfaces at the total ray distance to the source. This lets us handle sparse samplings of the input scene where reflection highlights do not always appear consistently in input images. We show interactive free-viewpoint rendering results from five scenes, one of which covers an area of more than 100 m2. Experimental results show that our method produces higher-quality renderings than a single large-capacity MLP and five recent neural proxy-geometry and voxel-based baseline methods. Our code and data are available at project webpage https://xchaowu.github.io/papers/scalable-nisr.",
    full_text: "# Scalable neural indoor scene rendering\n\n## Abstract\n\nWe propose a scalable neural scene reconstruction and rendering method to support distributed training and interactive rendering of large indoor scenes. Our representation is based on tiles. Tile appearances are trained in parallel through a background sampling strategy that augments each tile with distant scene information via a proxy global mesh. Each tile has two low-capacity MLPs: one for view-independent appearance (diffuse color and shading) and one for view-dependent appearance (specular highlights, reflections). We leverage the phenomena that complex view-dependent scene reflections can be attributed to virtual lights underneath surfaces at the total ray distance to the source. This lets us handle sparse samplings of the input scene where reflection highlights do not always appear consistently in input images. We show interactive free-viewpoint rendering results from five scenes, one of which covers an area of more than 100 m2. Experimental results show that our method produces higher-quality renderings than a single large-capacity MLP and five recent neural proxy-geometry and voxel-based baseline methods. Our code and data are available at project webpage https://xchaowu.github.io/papers/scalable-nisr.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nWe propose a scalable neural scene reconstruction and rendering method to support distributed training and interactive rendering of large indoor scenes.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Xiuchao Wu","Jiamin Xu","Zihan Zhu","H. Bao","Qi-Xing Huang","James Tompkin","Weiwei Xu"],
    arxiv_id: null,
    publication_date: new Date("2022-07-01T00:00:00.000Z"),
    venue: "ACM Transactions on Graphics",
    doi: "10.1145/3528223.3530153",
    citation_count: 46,
  },
  {
    title: "VoGE: A Differentiable Volume Renderer using Gaussian Ellipsoids for Analysis-by-Synthesis",
    abstract: "The Gaussian reconstruction kernels have been proposed by Westover (1990) and studied by the computer graphics community back in the 90s, which gives an alternative representation of object 3D geometry from meshes and point clouds. On the other hand, current state-of-the-art (SoTA) differentiable renderers, Liu et al. (2019), use rasterization to collect triangles or points on each image pixel and blend them based on the viewing distance. In this paper, we propose VoGE, which utilizes the volumetric Gaussian reconstruction kernels as geometric primitives. The VoGE rendering pipeline uses ray tracing to capture the nearest primitives and blends them as mixtures based on their volume density distributions along the rays. To efficiently render via VoGE, we propose an approximate closeform solution for the volume density aggregation and a coarse-to-fine rendering strategy. Finally, we provide a CUDA implementation of VoGE, which enables real-time level rendering with a competitive rendering speed in comparison to PyTorch3D. Quantitative and qualitative experiment results show VoGE outperforms SoTA counterparts when applied to various vision tasks, e.g., object pose estimation, shape/texture fitting, and occlusion reasoning. The VoGE library and demos are available at: https://github.com/Angtian/VoGE.",
    full_text: "# VoGE: A Differentiable Volume Renderer using Gaussian Ellipsoids for Analysis-by-Synthesis\n\n## Abstract\n\nThe Gaussian reconstruction kernels have been proposed by Westover (1990) and studied by the computer graphics community back in the 90s, which gives an alternative representation of object 3D geometry from meshes and point clouds. On the other hand, current state-of-the-art (SoTA) differentiable renderers, Liu et al. (2019), use rasterization to collect triangles or points on each image pixel and blend them based on the viewing distance. In this paper, we propose VoGE, which utilizes the volumetric Gaussian reconstruction kernels as geometric primitives. The VoGE rendering pipeline uses ray tracing to capture the nearest primitives and blends them as mixtures based on their volume density distributions along the rays. To efficiently render via VoGE, we propose an approximate closeform solution for the volume density aggregation and a coarse-to-fine rendering strategy. Finally, we provide a CUDA implementation of VoGE, which enables real-time level rendering with a competitive rendering speed in comparison to PyTorch3D. Quantitative and qualitative experiment results show VoGE outperforms SoTA counterparts when applied to various vision tasks, e.g., object pose estimation, shape/texture fitting, and occlusion reasoning. The VoGE library and demos are available at: https://github.com/Angtian/VoGE.\n\n## Introduction\n\n[Full text would be extracted from PDF in production]\n\nThis paper referenced by seminal 3dgs paper (foundational work).\n\n## Methodology\n\n[Paper content would be here]\n\n## Results\n\n[Paper results would be here]\n\n## Conclusion\n\nThe Gaussian reconstruction kernels have been proposed by Westover (1990) and studied by the computer graphics community back in the 90s, which gives an alternative representation of object 3D geometry from meshes and point clouds.\n\n## References\n\n[References would be extracted from PDF]",
    authors: ["Angtian Wang","Peng Wang","Jian Sun","Adam Kortylewski","A. Yuille"],
    arxiv_id: "2205.15401",
    publication_date: new Date("2022-05-30T00:00:00.000Z"),
    venue: "International Conference on Learning Representations",
    doi: "10.48550/arXiv.2205.15401",
    citation_count: 25,
  }
];

export default fetchedPapers;
