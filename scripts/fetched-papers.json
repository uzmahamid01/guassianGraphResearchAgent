[
  {
    "title": "Neural Point Catacaustics for Novel-View Synthesis of Reflections",
    "abstract": "View-dependent effects such as reflections pose a substantial challenge for image-based and neural rendering algorithms. Above all, curved reflectors are particularly hard, as they lead to highly non-linear reflection flows as the camera moves. We introduce a new point-based representation to compute Neural Point Catacaustics allowing novel-view synthesis of scenes with curved reflectors, from a set of casually-captured input photos. At the core of our method is a neural warp field that models catacaustic trajectories of reflections, so complex specular effects can be rendered using efficient point splatting in conjunction with a neural renderer. One of our key contributions is the explicit representation of reflections with a reflection point cloud which is displaced by the neural warp field, and a primary point cloud which is optimized to represent the rest of the scene. After a short manual annotation step, our approach allows interactive high-quality renderings of novel views with accurate reflection flow. Additionally, the explicit representation of reflection flow supports several forms of scene manipulation in captured scenes, such as reflection editing, cloning of specular objects, reflection tracking across views, and comfortable stereo viewing. We provide the source code and other supplemental material on https://repo-sam.inria.fr/fungraph/neural_catacaustics/",
    "authors": [
      "Georgios Kopanas",
      "Thomas Leimkühler",
      "G. Rainer",
      "Clément Jambon",
      "G. Drettakis"
    ],
    "year": 2022,
    "citation_count": 100,
    "arxiv_id": "2301.01087",
    "doi": "10.1145/3550454.3555497",
    "url": "https://www.semanticscholar.org/paper/d9e90c68ba1769d237e01c98c5de6a94d9c9087e",
    "venue": "ACM Transactions on Graphics",
    "publication_date": "2022-11-30",
    "relevance_score": 80,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Variable Bitrate Neural Fields",
    "abstract": "Neural approximations of scalar- and vector fields, such as signed distance functions and radiance fields, have emerged as accurate, high-quality representations. State-of-the-art results are obtained by conditioning a neural approximation with a lookup from trainable feature grids [Liu et al. 2020; Martel et al. 2021; Müller et al. 2022; Takikawa et al. 2021] that take on part of the learning task and allow for smaller, more efficient neural networks. Unfortunately, these feature grids usually come at the cost of significantly increased memory consumption compared to stand-alone neural network models. We present a dictionary method for compressing such feature grids, reducing their memory consumption by up to 100 × and permitting a multiresolution representation which can be useful for out-of-core streaming. We formulate the dictionary optimization as a vector-quantized auto-decoder problem which lets us learn end-to-end discrete neural representations in a space where no direct supervision is available and with dynamic topology and structure. Our source code is available at https://github.com/nv-tlabs/vqad.",
    "authors": [
      "Towaki Takikawa",
      "Alex Evans",
      "Jonathan Tremblay",
      "T. Müller",
      "M. McGuire",
      "Alec Jacobson",
      "S. Fidler"
    ],
    "year": 2022,
    "citation_count": 141,
    "arxiv_id": "2206.07707",
    "doi": "10.1145/3528233.3530727",
    "url": "https://www.semanticscholar.org/paper/d538645ba41ae2df38e63dfdd58958e0656a6292",
    "venue": "International Conference on Computer Graphics and Interactive Techniques",
    "publication_date": "2022-06-15",
    "relevance_score": 80,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Differentiable Point-Based Radiance Fields for Efficient View Synthesis",
    "abstract": "We propose a differentiable rendering algorithm for efficient novel view synthesis. By departing from volume-based representations in favor of a learned point representation, we improve on existing methods more than an order of magnitude in memory and runtime, both in training and inference. The method begins with a uniformly-sampled random point cloud and learns per-point position and view-dependent appearance, using a differentiable splat-based renderer to train the model to reproduce a set of input training images with the given pose. Our method is up to 300 × faster than NeRF in both training and inference, with only a marginal sacrifice in quality, while using less than 10 MB of memory for a static scene. For dynamic scenes, our method trains two orders of magnitude faster than STNeRF and renders at a near interactive rate, while maintaining high image quality and temporal coherence even without imposing any temporal-coherency regularizers.",
    "authors": [
      "Qian Zhang",
      "Seung-Hwan Baek",
      "S. Rusinkiewicz",
      "Felix Heide"
    ],
    "year": 2022,
    "citation_count": 106,
    "arxiv_id": "2205.14330",
    "doi": "10.1145/3550469.3555413",
    "url": "https://www.semanticscholar.org/paper/e14dad7cc93f5eb0d74b9078b62e6cdff3b70cab",
    "venue": "ACM SIGGRAPH Conference and Exhibition on Computer Graphics and Interactive Techniques in Asia",
    "publication_date": "2022-05-28",
    "relevance_score": 80,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "TensoRF: Tensorial Radiance Fields",
    "abstract": "We present TensoRF, a novel approach to model and reconstruct radiance fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a scene as a 4D tensor, which represents a 3D voxel grid with per-voxel multi-channel features. Our central idea is to factorize the 4D scene tensor into multiple compact low-rank tensor components. We demonstrate that applying traditional CP decomposition -- that factorizes tensors into rank-one components with compact vectors -- in our framework leads to improvements over vanilla NeRF. To further boost performance, we introduce a novel vector-matrix (VM) decomposition that relaxes the low-rank constraints for two modes of a tensor and factorizes tensors into compact vector and matrix factors. Beyond superior rendering quality, our models with CP and VM decompositions lead to a significantly lower memory footprint in comparison to previous and concurrent works that directly optimize per-voxel features. Experimentally, we demonstrate that TensoRF with CP decomposition achieves fast reconstruction (<30 min) with better rendering quality and even a smaller model size (<4 MB) compared to NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality and outperforms previous state-of-the-art methods, while reducing the reconstruction time (<10 min) and retaining a compact model size (<75 MB).",
    "authors": [
      "Anpei Chen",
      "Zexiang Xu",
      "Andreas Geiger",
      "Jingyi Yu",
      "Hao Su"
    ],
    "year": 2022,
    "citation_count": 1513,
    "arxiv_id": "2203.09517",
    "doi": "10.48550/arXiv.2203.09517",
    "url": "https://www.semanticscholar.org/paper/d267731870c41d977c9d51195c1e2fd018846949",
    "venue": "European Conference on Computer Vision",
    "publication_date": "2022-03-17",
    "relevance_score": 80,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Point-NeRF: Point-based Neural Radiance Fields",
    "abstract": "Volumetric neural rendering methods like NeRF [34] generate high-quality view synthesis results but are optimized per-scene leading to prohibitive reconstruction time. On the other hand, deep multi-view stereo methods can quickly reconstruct scene geometry via direct network inference. Point-NeRF combines the advantages of these two approaches by using neural 3D point clouds, with associated neural features, to model a radiance field. Point-NeRF can be rendered efficiently by aggregating neural point features near scene surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can be initialized via direct inference of a pre-trained deep network to produce a neural point cloud; this point cloud can be finetuned to surpass the visual quality of NeRF with 30× faster training time. Point-NeRF can be combined with other 3D re-construction methods and handles the errors and outliers in such methods via a novel pruning and growing mechanism.",
    "authors": [
      "Qiangeng Xu",
      "Zexiang Xu",
      "J. Philip",
      "Sai Bi",
      "Zhixin Shu",
      "Kalyan Sunkavalli",
      "U. Neumann"
    ],
    "year": 2022,
    "citation_count": 664,
    "arxiv_id": "2201.08845",
    "doi": "10.1109/CVPR52688.2022.00536",
    "url": "https://www.semanticscholar.org/paper/055e87ce418a83d6fd555b73aea0d838385dfa85",
    "venue": "Computer Vision and Pattern Recognition",
    "publication_date": "2022-01-21",
    "relevance_score": 80,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Instant neural graphics primitives with a multiresolution hash encoding",
    "abstract": "Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920×1080.",
    "authors": [
      "T. Müller",
      "Alex Evans",
      "Christoph Schied",
      "A. Keller"
    ],
    "year": 2022,
    "citation_count": 4848,
    "arxiv_id": "2201.05989",
    "doi": "10.1145/3528223.3530127",
    "url": "https://www.semanticscholar.org/paper/60e69982ef2920596c6f31d6fd3ca5e9591f3db6",
    "venue": "ACM Transactions on Graphics",
    "publication_date": "2022-01-16",
    "relevance_score": 80,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Plenoxels: Radiance Fields without Neural Networks",
    "abstract": "We introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels.",
    "authors": [
      "Alex Yu",
      "Sara Fridovich-Keil",
      "Matthew Tancik",
      "Qinhong Chen",
      "B. Recht",
      "Angjoo Kanazawa"
    ],
    "year": 2021,
    "citation_count": 1929,
    "arxiv_id": "2112.05131",
    "doi": "10.1109/CVPR52688.2022.00542",
    "url": "https://www.semanticscholar.org/paper/e91f73aaef155391b5b07e6612f5346dea888f64",
    "venue": "Computer Vision and Pattern Recognition",
    "publication_date": "2021-12-09",
    "relevance_score": 80,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Neural Fields in Visual Computing and Beyond",
    "abstract": "Recent advances in machine learning have led to increased interest in solving visual computing problems using methods that employ coordinate‐based neural networks. These methods, which we call neural fields, parameterize physical properties of scenes or objects across space and time. They have seen widespread success in problems such as 3D shape and image synthesis, animation of human bodies, 3D reconstruction, and pose estimation. Rapid progress has led to numerous papers, but a consolidation of the discovered knowledge has not yet emerged. We provide context, mathematical grounding, and a review of over 250 papers in the literature on neural fields. In Part I, we focus on neural field techniques by identifying common components of neural field methods, including different conditioning, representation, forward map, architecture, and manipulation methods. In Part II, we focus on applications of neural fields to different problems in visual computing, and beyond (e.g., robotics, audio). Our review shows the breadth of topics already covered in visual computing, both historically and in current incarnations, and highlights the improved quality, flexibility, and capability brought by neural field methods. Finally, we present a companion website that acts as a living database that can be continually updated by the community.",
    "authors": [
      "Yiheng Xie",
      "Towaki Takikawa",
      "Shunsuke Saito",
      "O. Litany",
      "Shiqin Yan",
      "Numair Khan",
      "Federico Tombari",
      "James Tompkin",
      "Vincent Sitzmann",
      "Srinath Sridhar"
    ],
    "year": 2021,
    "citation_count": 715,
    "arxiv_id": "2111.11426",
    "doi": "10.1111/cgf.14505",
    "url": "https://www.semanticscholar.org/paper/521026c6097b0c0b44cb97b8a3952261527bea41",
    "venue": "Computer graphics forum (Print)",
    "publication_date": "2021-11-22",
    "relevance_score": 80,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction",
    "abstract": "We present a super-fast convergence approach to reconstructing the per-scene radiance field from a set of images that capture the scene with known poses. This task, which is often applied to novel view synthesis, is recently revolution-ized by Neural Radiance Field (NeRF) for its state-of-the-art quality and fiexibility. However, NeRF and its variants require a lengthy training time ranging from hours to days for a single scene. In contrast, our approach achieves NeRF-comparable quality and converges rapidly from scratch in less than 15 minutes with a single GPU. We adopt a representation consisting of a density voxel grid for scene geometry and a feature voxel grid with a shallow network for complex view-dependent appearance. Modeling with explicit and discretized volume representations is not new, but we propose two simple yet non-trivial techniques that contribute to fast convergence speed and high-quality output. First, we introduce the post-activation interpolation on voxel density, which is capable of producing sharp surfaces in lower grid resolution. Second, direct voxel density optimization is prone to suboptimal geometry solutions, so we robustify the optimization process by imposing several priors. Finally, evaluation on five inward-facing benchmarks shows that our method matches, if not surpasses, NeRF's quality, yet it only takes about 15 minutes to train from scratch for a new scene. Code: https://github.com/sunset1995/DirectVoxGO.",
    "authors": [
      "Cheng Sun",
      "Min Sun",
      "Hwann-Tzong Chen"
    ],
    "year": 2021,
    "citation_count": 1230,
    "arxiv_id": "2111.11215",
    "doi": "10.1109/CVPR52688.2022.00538",
    "url": "https://www.semanticscholar.org/paper/4f7eb65f8d3c1eeb97e30f7ac68977ff16e1e942",
    "venue": "Computer Vision and Pattern Recognition",
    "publication_date": "2021-11-22",
    "relevance_score": 80,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Advances in Neural Rendering",
    "abstract": "Synthesizing photo‐realistic images and videos is at the heart of computer graphics and has been the focus of decades of research. Traditionally, synthetic images of a scene are generated using rendering algorithms such as rasterization or ray tracing, which take specifically defined representations of geometry and material properties as input. Collectively, these inputs define the actual scene and what is rendered, and are referred to as the scene representation (where a scene consists of one or more objects). Example scene representations are triangle meshes with accompanied textures (e.g., created by an artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g., from a CT scan), or implicit surface functions (e.g., truncated signed distance fields). The reconstruction of such a scene representation from observations using differentiable rendering losses is known as inverse graphics or inverse rendering. Neural rendering is closely related, and combines ideas from classical computer graphics and machine learning to create algorithms for synthesizing images from real‐world observations. Neural rendering is a leap forward towards the goal of synthesizing photo‐realistic image and video content. In recent years, we have seen immense progress in this field through hundreds of publications that show different ways to inject learnable components into the rendering pipeline. This state‐of‐the‐art report on advances in neural rendering focuses on methods that combine classical rendering principles with learned 3D scene representations, often now referred to as neural scene representations. A key advantage of these methods is that they are 3D‐consistent by design, enabling applications such as novel viewpoint synthesis of a captured scene. In addition to methods that handle static scenes, we cover neural scene representations for modeling non‐rigidly deforming objects and scene editing and composition. While most of these approaches are scene‐specific, we also discuss techniques that generalize across object classes and can be used for generative tasks. In addition to reviewing these state‐of‐the‐art methods, we provide an overview of fundamental concepts and definitions used in the current literature. We conclude with a discussion on open challenges and social implications.",
    "authors": [
      "A. Tewari",
      "O. Fried",
      "Justus Thies",
      "V. Sitzmann",
      "S. Lombardi",
      "Z. Xu",
      "T. Simon",
      "M. Nießner",
      "E. Tretschk",
      "L. Liu",
      "B. Mildenhall",
      "P. Srinivasan",
      "R. Pandey",
      "S. Orts-Escolano",
      "S. Fanello",
      "M. Guo",
      "Gordon Wetzstein",
      "J. Zhu",
      "C. Theobalt",
      "M. Agrawala",
      "D. B. Goldman",
      "M. Zollhöfer"
    ],
    "year": 2021,
    "citation_count": 500,
    "arxiv_id": "2111.05849",
    "doi": "10.1111/cgf.14507",
    "url": "https://www.semanticscholar.org/paper/0b5b6598e3e108147842f35ff66a95d989f9ec89",
    "venue": "SIGGRAPH Courses",
    "publication_date": "2021-07-21",
    "relevance_score": 80,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Point‐Based Neural Rendering with Per‐View Optimization",
    "abstract": "There has recently been great interest in neural rendering methods. Some approaches use 3D geometry reconstructed with Multi‐View Stereo (MVS) but cannot recover from the errors of this process, while others directly learn a volumetric neural representation, but suffer from expensive training and inference. We introduce a general approach that is initialized with MVS, but allows further optimization of scene properties in the space of input views, including depth and reprojected features, resulting in improved novel‐view synthesis. A key element of our approach is our new differentiable point‐based pipeline, based on bi‐directional Elliptical Weighted Average splatting, a probabilistic depth test and effective camera selection. We use these elements together in our neural renderer, that outperforms all previous methods both in quality and speed in almost all scenes we tested. Our pipeline can be applied to multi‐view harmonization and stylization in addition to novel‐view synthesis.",
    "authors": [
      "Georgios Kopanas",
      "J. Philip",
      "Thomas Leimkühler",
      "G. Drettakis"
    ],
    "year": 2021,
    "citation_count": 235,
    "arxiv_id": "2109.02369",
    "doi": "10.1111/cgf.14339",
    "url": "https://www.semanticscholar.org/paper/2ac30b5ba83c1fabb0e864d4d3c93a70367accc6",
    "venue": "Computer graphics forum (Print)",
    "publication_date": "2021-07-01",
    "relevance_score": 80,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Baking Neural Radiance Fields for Real-Time View Synthesis",
    "abstract": "Neural volumetric representations such as Neural Radiance Fields (NeRF) have emerged as a compelling technique for learning to represent 3D scenes from images with the goal of rendering photorealistic images of the scene from unobserved viewpoints. However, NeRF’s computational requirements are prohibitive for real-time applications: rendering views from a trained NeRF requires querying a multilayer perceptron (MLP) hundreds of times per ray. We present a method to train a NeRF, then precompute and store (i.e. \"bake\") it as a novel representation called a Sparse Neural Radiance Grid (SNeRG) that enables real-time rendering on commodity hardware. To achieve this, we introduce 1) a reformulation of NeRF’s architecture, and 2) a sparse voxel grid representation with learned feature vectors. The resulting scene representation retains NeRF’s ability to render fine geometric details and view-dependent appearance, is compact (averaging less than 90 MB per scene), and can be rendered in real-time (higher than 30 frames per second on a laptop GPU). Actual screen captures are shown in our video.",
    "authors": [
      "Peter Hedman",
      "Pratul P. Srinivasan",
      "B. Mildenhall",
      "J. Barron",
      "P. Debevec"
    ],
    "year": 2021,
    "citation_count": 613,
    "arxiv_id": "2103.14645",
    "doi": "10.1109/ICCV48922.2021.00582",
    "url": "https://www.semanticscholar.org/paper/8bd70d3dcfa295d9710922c34c1a9eeb0be48b94",
    "venue": "IEEE International Conference on Computer Vision",
    "publication_date": "2021-03-26",
    "relevance_score": 80,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs",
    "abstract": "NeRF synthesizes novel views of a scene with unprecedented quality by fitting a neural radiance field to RGB images. However, NeRF requires querying a deep Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering times, even on modern GPUs. In this paper, we demonstrate that real-time rendering is possible by utilizing thousands of tiny MLPs instead of one single large MLP. In our setting, each individual MLP only needs to represent parts of the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining this divide-and-conquer strategy with further optimizations, rendering is accelerated by three orders of magnitude compared to the original NeRF model without incurring high storage costs. Further, using teacher-student distillation for training, we show that this speed-up can be achieved without sacrificing visual quality.",
    "authors": [
      "Christian Reiser",
      "Songyou Peng",
      "Yiyi Liao",
      "Andreas Geiger"
    ],
    "year": 2021,
    "citation_count": 901,
    "arxiv_id": "2103.13744",
    "doi": "10.1109/ICCV48922.2021.01407",
    "url": "https://www.semanticscholar.org/paper/c041aaed581616e122e790dd2769337216df3d8d",
    "venue": "IEEE International Conference on Computer Vision",
    "publication_date": "2021-03-25",
    "relevance_score": 80,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "PlenOctrees for Real-time Rendering of Neural Radiance Fields",
    "abstract": "We introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent effects. Our method can render 800×800 images at more than 150 FPS, which is over 3000 times faster than conventional NeRFs. We do so without sacrificing quality while preserving the ability of NeRFs to perform free-viewpoint rendering of scenes with arbitrary geometry and view-dependent effects. Real-time performance is achieved by pre-tabulating the NeRF into a PlenOctree. In order to preserve view-dependent effects such as specularities, we factorize the appearance via closed-form spherical basis functions. Specifically, we show that it is possible to train NeRFs to predict a spherical harmonic representation of radiance, removing the viewing direction as an input to the neural network. Furthermore, we show that PlenOctrees can be directly optimized to further minimize the reconstruction loss, which leads to equal or better quality compared to competing methods. Moreover, this octree optimization step can be used to reduce the training time, as we no longer need to wait for the NeRF training to converge fully. Our real-time neural rendering approach may potentially enable new applications such as 6-DOF industrial and product visualizations, as well as next generation AR/VR systems. PlenOctrees are amenable to in-browser rendering as well; please visit the project page for the interactive online demo, as well as video and code: https://alexyu.net/plenoctrees.",
    "authors": [
      "Alex Yu",
      "Ruilong Li",
      "Matthew Tancik",
      "Hao Li",
      "Ren Ng",
      "Angjoo Kanazawa"
    ],
    "year": 2021,
    "citation_count": 1206,
    "arxiv_id": "2103.14024",
    "doi": "10.1109/ICCV48922.2021.00570",
    "url": "https://www.semanticscholar.org/paper/5744fcc21b40327f7ad710de7d947d4584c53012",
    "venue": "IEEE International Conference on Computer Vision",
    "publication_date": "2021-03-25",
    "relevance_score": 80,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields",
    "abstract": "The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call \"mip-NeRF\" (à la \"mipmap\"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF’s ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22× faster.",
    "authors": [
      "J. Barron",
      "B. Mildenhall",
      "Matthew Tancik",
      "Peter Hedman",
      "Ricardo Martin-Brualla",
      "Pratul P. Srinivasan"
    ],
    "year": 2021,
    "citation_count": 2355,
    "arxiv_id": "2103.13415",
    "doi": "10.1109/ICCV48922.2021.00580",
    "url": "https://www.semanticscholar.org/paper/21336e57dc2ab9ae2171a0f6c35f7d1aba584796",
    "venue": "IEEE International Conference on Computer Vision",
    "publication_date": "2021-03-24",
    "relevance_score": 80,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "FastNeRF: High-Fidelity Neural Rendering at 200FPS",
    "abstract": "Recent work on Neural Radiance Fields (NeRF) showed how neural networks can be used to encode complex 3D environments that can be rendered photorealistically from novel viewpoints. Rendering these images is very computationally demanding and recent improvements are still a long way from enabling interactive rates, even on high-end hardware. Motivated by scenarios on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based system capable of rendering high fidelity photorealistic images at 200Hz on a high-end consumer GPU. The core of our method is a graphics-inspired factorization that allows for (i) compactly caching a deep radiance map at each position in space, (ii) efficiently querying that map using ray directions to estimate the pixel values in the rendered image. Extensive experiments show that the proposed method is 3000 times faster than the original NeRF algorithm and at least an order of magnitude faster than existing work on accelerating NeRF, while maintaining visual quality and extensibility.",
    "authors": [
      "Stephan J. Garbin",
      "Marek Kowalski",
      "Matthew Johnson",
      "J. Shotton",
      "Julien P. C. Valentin"
    ],
    "year": 2021,
    "citation_count": 720,
    "arxiv_id": "2103.10380",
    "doi": "10.1109/ICCV48922.2021.01408",
    "url": "https://www.semanticscholar.org/paper/c0ccfbaf073c91e68ccbc57af2114c72c0d0427d",
    "venue": "IEEE International Conference on Computer Vision",
    "publication_date": "2021-03-18",
    "relevance_score": 80,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Mixture of volumetric primitives for efficient neural rendering",
    "abstract": "Real-time rendering and animation of humans is a core function in games, movies, and telepresence applications. Existing methods have a number of drawbacks we aim to address with our work. Triangle meshes have difficulty modeling thin structures like hair, volumetric representations like Neural Volumes are too low-resolution given a reasonable memory budget, and high-resolution implicit representations like Neural Radiance Fields are too slow for use in real-time applications. We present Mixture of Volumetric Primitives (MVP), a representation for rendering dynamic 3D content that combines the completeness of volumetric representations with the efficiency of primitive-based rendering, e.g., point-based or mesh-based methods. Our approach achieves this by leveraging spatially shared computation with a convolutional architecture and by minimizing computation in empty regions of space with volumetric primitives that can move to cover only occupied regions. Our parameterization supports the integration of correspondence and tracking constraints, while being robust to areas where classical tracking fails, such as around thin or translucent structures and areas with large topological variability. MVP is a hybrid that generalizes both volumetric and primitive-based representations. Through a series of extensive experiments we demonstrate that it inherits the strengths of each, while avoiding many of their limitations. We also compare our approach to several state-of-the-art methods and demonstrate that MVP produces superior results in terms of quality and runtime performance.",
    "authors": [
      "Stephen Lombardi",
      "T. Simon",
      "Gabriel Schwartz",
      "Michael Zollhoefer",
      "Yaser Sheikh",
      "Jason M. Saragih"
    ],
    "year": 2021,
    "citation_count": 326,
    "arxiv_id": "2103.01954",
    "doi": "10.1145/3476576.3476608",
    "url": "https://www.semanticscholar.org/paper/43b77c7e55a206c7de3449ec7e898bc6f4986734",
    "venue": "ACM Transactions on Graphics",
    "publication_date": "2021-03-02",
    "relevance_score": 80,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes",
    "abstract": "Neural signed distance functions (SDFs) are emerging as an effective representation for 3D shapes. State-of-the-art methods typically encode the SDF with a large, fixed-size neural network to approximate complex shapes with implicit surfaces. Rendering with these large networks is, however, computationally expensive since it requires many forward passes through the network for every pixel, making these representations impractical for real-time graphics. We introduce an efficient neural representation that, for the first time, enables real-time rendering of high-fidelity neural SDFs, while achieving state-of-the-art geometry reconstruction quality. We represent implicit surfaces using an octree-based feature volume which adaptively fits shapes with multiple discrete levels of detail (LODs), and enables continuous LOD with SDF interpolation. We further develop an efficient algorithm to directly render our novel neural SDF representation in real-time by querying only the necessary LODs with sparse octree traversal. We show that our representation is 2–3 orders of magnitude more efficient in terms of rendering speed compared to previous works. Furthermore, it produces state-of-the-art reconstruction quality for complex shapes under both 3D geometric and 2D image-space metrics.",
    "authors": [
      "Towaki Takikawa",
      "Joey Litalien",
      "K. Yin",
      "Karsten Kreis",
      "Charles T. Loop",
      "D. Nowrouzezahrai",
      "Alec Jacobson",
      "M. McGuire",
      "S. Fidler"
    ],
    "year": 2021,
    "citation_count": 573,
    "arxiv_id": "2101.10994",
    "doi": "10.1109/CVPR46437.2021.01120",
    "url": "https://www.semanticscholar.org/paper/6ca0a80111fed68b8a5b7eac5ecdc3d258f0fea6",
    "venue": "Computer Vision and Pattern Recognition",
    "publication_date": "2021-01-26",
    "relevance_score": 80,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Pulsar: Efficient Sphere-based Neural Rendering",
    "abstract": "We propose Pulsar, an efficient sphere-based differentiable rendering module that is orders of magnitude faster than competing techniques, modular, and easy-to-use due to its tight integration with PyTorch. Differentiable rendering is the foundation for modern neural rendering approaches, since it enables end-to-end training of 3D scene representations from image observations. However, gradient-based optimization of neural mesh, voxel, or function representations suffers from multiple challenges, i.e., topological in-consistencies, high memory footprints, or slow rendering speeds. To alleviate these problems, Pulsar employs: 1) a sphere-based scene representation, 2) a modular, efficient differentiable projection operation, and 3) (optional) neural shading. Pulsar executes orders of magnitude faster than existing techniques and allows real-time rendering and optimization of representations with millions of spheres. Using spheres for the scene representation, unprecedented speed is obtained while avoiding topology problems. Pulsar is fully differentiable and thus enables a plethora of applications, ranging from 3D reconstruction to neural rendering.",
    "authors": [
      "Christoph Lassner",
      "M. Zollhöfer"
    ],
    "year": 2021,
    "citation_count": 202,
    "arxiv_id": null,
    "doi": "10.1109/CVPR46437.2021.00149",
    "url": "https://www.semanticscholar.org/paper/ffba7fc5a4b350d50c406b8e52a57cc31ac971dd",
    "venue": "Computer Vision and Pattern Recognition",
    "publication_date": "2021-06-01",
    "relevance_score": 75,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Free View Synthesis",
    "abstract": "We present a method for novel view synthesis from input images that are freely distributed around a scene. Our method does not rely on a regular arrangement of input views, can synthesize images for free camera movement through the scene, and works for general scenes with unconstrained geometric layouts. We calibrate the input images via SfM and erect a coarse geometric scaffold via MVS. This scaffold is used to create a proxy depth map for a novel view of the scene. Based on this depth map, a recurrent encoder-decoder network processes reprojected features from nearby views and synthesizes the new view. Our network does not need to be optimized for a given scene. After training on a dataset, it works in previously unseen environments with no fine-tuning or per-scene optimization. We evaluate the presented approach on challenging real-world datasets, including Tanks and Temples, where we demonstrate successful view synthesis for the first time and substantially outperform prior and concurrent work.",
    "authors": [
      "Gernot Riegler",
      "V. Koltun"
    ],
    "year": 2020,
    "citation_count": 376,
    "arxiv_id": "2008.05511",
    "doi": "10.1007/978-3-030-58529-7_37",
    "url": "https://www.semanticscholar.org/paper/49fae04a4e9383080788759f63dba75c86bd21b0",
    "venue": "European Conference on Computer Vision",
    "publication_date": "2020-08-12",
    "relevance_score": 75,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "SynSin: End-to-End View Synthesis From a Single Image",
    "abstract": "View synthesis allows for the generation of new views of a scene given one or more images. This is challenging; it requires comprehensively understanding the 3D scene from images. As a result, current methods typically use multiple images, train on ground-truth depth, or are limited to synthetic data. We propose a novel end-to-end model for this task using a single image at test time; it is trained on real images without any ground-truth 3D information. To this end, we introduce a novel differentiable point cloud renderer that is used to transform a latent 3D point cloud of features into the target view. The projected features are decoded by our refinement network to inpaint missing regions and generate a realistic output image. The 3D component inside of our generative model allows for interpretable manipulation of the latent feature space at test time, e.g. we can animate trajectories from a single image. Additionally, we can generate high resolution images and generalise to other input resolutions. We outperform baselines and prior work on the Matterport, Replica, and RealEstate10K datasets.",
    "authors": [
      "Olivia Wiles",
      "Georgia Gkioxari",
      "R. Szeliski",
      "Justin Johnson"
    ],
    "year": 2019,
    "citation_count": 510,
    "arxiv_id": "1912.08804",
    "doi": "10.1109/CVPR42600.2020.00749",
    "url": "https://www.semanticscholar.org/paper/30e911f84f00423625f50ec70b9056ffc3d5b8ca",
    "venue": "Computer Vision and Pattern Recognition",
    "publication_date": "2019-12-18",
    "relevance_score": 75,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Neural Point-Based Graphics",
    "abstract": "We present a new point-based approach for modeling the appearance of real scenes. The approach uses a raw point cloud as the geometric representation of a scene, and augments each point with a learnable neural descriptor that encodes local geometry and appearance. A deep rendering network is learned in parallel with the descriptors, so that new views of the scene can be obtained by passing the rasterizations of a point cloud from new viewpoints through this network. The input rasterizations use the learned descriptors as point pseudo-colors. We show that the proposed approach can be used for modeling complex scenes and obtaining their photorealistic views, while avoiding explicit surface estimation and meshing. In particular, compelling results are obtained for scene scanned using hand-held commodity RGB-D sensors as well as standard RGB cameras even in the presence of objects that are challenging for standard mesh-based modeling.",
    "authors": [
      "Kara-Ali Aliev",
      "Dmitry Ulyanov",
      "V. Lempitsky"
    ],
    "year": 2019,
    "citation_count": 464,
    "arxiv_id": "1906.08240",
    "doi": "10.1007/978-3-030-58542-6_42",
    "url": "https://www.semanticscholar.org/paper/eb516fbfe0d49d2e903ff4605ec63cd7983d059a",
    "venue": "European Conference on Computer Vision",
    "publication_date": "2019-06-19",
    "relevance_score": 75,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Differentiable surface splatting for point-based geometry processing",
    "abstract": "We propose Differentiable Surface Splatting (DSS), a high-fidelity differentiable renderer for point clouds. Gradients for point locations and normals are carefully designed to handle discontinuities of the rendering function. Regularization terms are introduced to ensure uniform distribution of the points on the underlying surface. We demonstrate applications of DSS to inverse rendering for geometry synthesis and denoising, where large scale topological changes, as well as small scale detail modifications, are accurately and robustly handled without requiring explicit connectivity, outperforming state-of-the-art techniques. The data and code are at https://github.com/yifita/DSS.",
    "authors": [
      "Yifan Wang",
      "Felice Serena",
      "Shihao Wu",
      "Cengiz Öztireli",
      "O. Sorkine-Hornung"
    ],
    "year": 2019,
    "citation_count": 345,
    "arxiv_id": "1906.04173",
    "doi": "10.1145/3355089.3356513",
    "url": "https://www.semanticscholar.org/paper/15a8a5acbe771477f83366bcd60897894c3cdb9b",
    "venue": "ACM Transactions on Graphics",
    "publication_date": "2019-06-10",
    "relevance_score": 75,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Deferred neural rendering",
    "abstract": "The modern computer graphics pipeline can synthesize images at remarkable visual quality; however, it requires well-defined, high-quality 3D content as input. In this work, we explore the use of imperfect 3D content, for instance, obtained from photo-metric reconstructions with noisy and incomplete surface geometry, while still aiming to produce photo-realistic (re-)renderings. To address this challenging problem, we introduce Deferred Neural Rendering, a new paradigm for image synthesis that combines the traditional graphics pipeline with learnable components. Specifically, we propose Neural Textures, which are learned feature maps that are trained as part of the scene capture process. Similar to traditional textures, neural textures are stored as maps on top of 3D mesh proxies; however, the high-dimensional feature maps contain significantly more information, which can be interpreted by our new deferred neural rendering pipeline. Both neural textures and deferred neural renderer are trained end-to-end, enabling us to synthesize photo-realistic images even when the original 3D content was imperfect. In contrast to traditional, black-box 2D generative neural networks, our 3D representation gives us explicit control over the generated output, and allows for a wide range of application domains. For instance, we can synthesize temporally-consistent video re-renderings of recorded 3D scenes as our representation is inherently embedded in 3D space. This way, neural textures can be utilized to coherently re-render or manipulate existing video content in both static and dynamic environments at real-time rates. We show the effectiveness of our approach in several experiments on novel view synthesis, scene editing, and facial reenactment, and compare to state-of-the-art approaches that leverage the standard graphics pipeline as well as conventional generative neural networks.",
    "authors": [
      "Justus Thies",
      "M. Zollhöfer",
      "M. Nießner"
    ],
    "year": 2019,
    "citation_count": 734,
    "arxiv_id": "1904.12356",
    "doi": "10.1145/3306346.3323035",
    "url": "https://www.semanticscholar.org/paper/1795a62eb1bfd952c73acb18ca81f174b0042cae",
    "venue": "ACM Transactions on Graphics",
    "publication_date": "2019-04-28",
    "relevance_score": 75,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "DeepVoxels: Learning Persistent 3D Feature Embeddings",
    "abstract": "In this work, we address the lack of 3D understanding of generative neural networks by introducing a persistent 3D feature embedding for view synthesis. To this end, we propose DeepVoxels, a learned representation that encodes the view-dependent appearance of a 3D scene without having to explicitly model its geometry. At its core, our approach is based on a Cartesian 3D grid of persistent embedded features that learn to make use of the underlying 3D scene structure. Our approach combines insights from 3D geometric computer vision with recent advances in learning image-to-image mappings based on adversarial loss functions. DeepVoxels is supervised, without requiring a 3D reconstruction of the scene, using a 2D re-rendering loss and enforces perspective and multi-view geometry in a principled manner. We apply our persistent 3D scene representation to the problem of novel view synthesis demonstrating high-quality results for a variety of challenging scenes.",
    "authors": [
      "V. Sitzmann",
      "Justus Thies",
      "Felix Heide",
      "M. Nießner",
      "Gordon Wetzstein",
      "M. Zollhöfer"
    ],
    "year": 2018,
    "citation_count": 705,
    "arxiv_id": "1812.01024",
    "doi": "10.1109/CVPR.2019.00254",
    "url": "https://www.semanticscholar.org/paper/e2d3abc7008a269880918ee7d903a55d06acdd55",
    "venue": "Computer Vision and Pattern Recognition",
    "publication_date": "2018-12-03",
    "relevance_score": 70,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Escaping Plato’s Cave: 3D Shape From Adversarial Rendering",
    "abstract": "We introduce PlatonicGAN to discover the 3D structure of an object class from an unstructured collection of 2D images, i.e., where no relation between photos is known, except that they are showing instances of the same category. The key idea is to train a deep neural network to generate 3D shapes which, when rendered to images, are indistinguishable from ground truth images (for a discriminator) under various camera poses. Discriminating 2D images instead of 3D shapes allows tapping into unstructured 2D photo collections instead of relying on curated (e.g., aligned, annotated, etc.) 3D data sets. To establish constraints between 2D image observation and their 3D interpretation, we suggest a family of rendering layers that are effectively differentiable. This family includes visual hull, absorption-only (akin to x-ray), and emission-absorption. We can successfully reconstruct 3D shapes from unstructured 2D images and extensively evaluate PlatonicGAN on a range of synthetic and real data sets achieving consistent improvements over baseline methods. We further show that PlatonicGAN can be combined with 3D supervision to improve on and in some cases even surpass the quality of 3D-supervised methods.",
    "authors": [
      "P. Henzler",
      "N. Mitra",
      "Tobias Ritschel"
    ],
    "year": 2018,
    "citation_count": 244,
    "arxiv_id": "1811.11606",
    "doi": "10.1109/ICCV.2019.01008",
    "url": "https://www.semanticscholar.org/paper/eb21d7e724b037e97ba657a5becb36fc2e7330d8",
    "venue": "IEEE International Conference on Computer Vision",
    "publication_date": "2018-11-28",
    "relevance_score": 70,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Deep blending for free-viewpoint image-based rendering",
    "abstract": "Free-viewpoint image-based rendering (IBR) is a standing challenge. IBR methods combine warped versions of input photos to synthesize a novel view. The image quality of this combination is directly affected by geometric inaccuracies of multi-view stereo (MVS) reconstruction and by view- and image-dependent effects that produce artifacts when contributions from different input views are blended. We present a new deep learning approach to blending for IBR, in which we use held-out real image data to learn blending weights to combine input photo contributions. Our Deep Blending method requires us to address several challenges to achieve our goal of interactive free-viewpoint IBR navigation. We first need to provide sufficiently accurate geometry so the Convolutional Neural Network (CNN) can succeed in finding correct blending weights. We do this by combining two different MVS reconstructions with complementary accuracy vs. completeness tradeoffs. To tightly integrate learning in an interactive IBR system, we need to adapt our rendering algorithm to produce a fixed number of input layers that can then be blended by the CNN. We generate training data with a variety of captured scenes, using each input photo as ground truth in a held-out approach. We also design the network architecture and the training loss to provide high quality novel view synthesis, while reducing temporal flickering artifacts. Our results demonstrate free-viewpoint IBR in a wide variety of scenes, clearly surpassing previous methods in visual quality, especially when moving far from the input cameras.",
    "authors": [
      "Peter Hedman",
      "J. Philip",
      "True Price",
      "Jan-Michael Frahm",
      "G. Drettakis",
      "G. Brostow"
    ],
    "year": 2018,
    "citation_count": 672,
    "arxiv_id": null,
    "doi": "10.1145/3272127.3275084",
    "url": "https://www.semanticscholar.org/paper/30453b89e7d83f8142e5010a0998175bffc56025",
    "venue": "ACM Transactions on Graphics",
    "publication_date": "2018-12-04",
    "relevance_score": 65,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Comments on \"Compression of 3D Point Clouds Using a Region-Adaptive Hierarchical Transform\"",
    "abstract": "The recently introduced coder based on region-adaptive hierarchical transform (RAHT) for the compression of point clouds attributes, was shown to have a performance competitive with the state-of-the-art, while being much less complex. In the paper \"Compression of 3D Point Clouds Using a Region-Adaptive Hierarchical Transform\", top performance was achieved using arithmetic coding (AC), while adaptive run-length Golomb-Rice (RLGR) coding was presented as a lower-performance lower-complexity alternative. However, we have found that by reordering the RAHT coefficients we can largely increase the runs of zeros and significantly increase the performance of the RLGR-based RAHT coder. As a result, the new coder, using ordered coefficients, was shown to outperform all other coders, including AC-based RAHT, at an even lower computational cost. We present new results and plots that should enhance those in the work of Queiroz and Chou to include the new results for RLGR-RAHT. We risk to say, based on the results herein, that RLGR-RAHT with sorted coefficients is the new state-of-the-art in point cloud compression.",
    "authors": [
      "Gustavo P. Sandri",
      "Ricado L. de Queiroz",
      "P. Chou"
    ],
    "year": 2018,
    "citation_count": 90,
    "arxiv_id": "1805.09146",
    "doi": null,
    "url": "https://www.semanticscholar.org/paper/5388bdf2302ab91493a87021bf9a328455324170",
    "venue": "",
    "publication_date": "2018-05-23",
    "relevance_score": 65,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Tanks and temples",
    "abstract": "We present a benchmark for image-based 3D reconstruction. The benchmark sequences were acquired outside the lab, in realistic conditions. Ground-truth data was captured using an industrial laser scanner. The benchmark includes both outdoor scenes and indoor environments. High-resolution video sequences are provided as input, supporting the development of novel pipelines that take advantage of video input to increase reconstruction fidelity. We report the performance of many image-based 3D reconstruction pipelines on the new benchmark. The results point to exciting challenges and opportunities for future work.",
    "authors": [
      "A. Knapitsch",
      "Jaesik Park",
      "Qian-Yi Zhou",
      "V. Koltun"
    ],
    "year": 2017,
    "citation_count": 682,
    "arxiv_id": null,
    "doi": "10.1145/3072959.3073599",
    "url": "https://www.semanticscholar.org/paper/4287fca41f650f9a953a976136d0f9d271ac5c37",
    "venue": "ACM Transactions on Graphics",
    "publication_date": "2017-07-20",
    "relevance_score": 65,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Soft 3D reconstruction for view synthesis",
    "abstract": "Abstract not available",
    "authors": [
      "Eric Penner",
      "Li Zhang"
    ],
    "year": 2017,
    "citation_count": 400,
    "arxiv_id": null,
    "doi": "10.1145/3130800.3130855",
    "url": "https://www.semanticscholar.org/paper/45b9f6d454a1b6b47ab5571034463d358c9e4c65",
    "venue": "ACM Transactions on Graphics",
    "publication_date": "2017-11-20",
    "relevance_score": 55,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Software Rasterization of 2 Billion Points in Real Time",
    "abstract": "The accelerated collection of detailed real-world 3D data in the form of ever-larger point clouds is sparking a demand for novel visualization techniques that are capable of rendering billions of point primitives in real-time. We propose a software rasterization pipeline for point clouds that is capable of rendering up to two billion points in real-time (60 FPS) on commodity hardware. Improvements over the state of the art are achieved by batching points, enabling a number of batch-level optimizations before rasterizing them within the same rendering pass. These optimizations include frustum culling, level-of-detail (LOD) rendering, and choosing the appropriate coordinate precision for a given batch of points directly within a compute workgroup. Adaptive coordinate precision, in conjunction with visibility buffers, reduces the required data for the majority of points to just four bytes, making our approach several times faster than the bandwidth-limited state of the art. Furthermore, support for LOD rendering makes our software rasterization approach suitable for rendering arbitrarily large point clouds, and to meet the elevated performance demands of virtual reality applications.",
    "authors": [
      "Markus Schütz",
      "Bernhard Kerbl",
      "M. Wimmer"
    ],
    "year": 2022,
    "citation_count": 42,
    "arxiv_id": "2204.01287",
    "doi": "10.1145/3543863",
    "url": "https://www.semanticscholar.org/paper/a8bed1d0b5fff62f3b8d78bceb463f39f8557248",
    "venue": "Proceedings of the ACM on Computer Graphics and Interactive Techniques",
    "publication_date": "2022-04-04",
    "relevance_score": 51,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Online Submission ID: 0184 Photo Tourism: Exploring Photo Collections in 3D",
    "abstract": "Abstract not available",
    "authors": [],
    "year": 0,
    "citation_count": 3753,
    "arxiv_id": null,
    "doi": null,
    "url": "https://www.semanticscholar.org/paper/a31e9d09d90261fc68acffe097df592cfdcb7706",
    "venue": "",
    "publication_date": null,
    "relevance_score": 50,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Eurographics Symposium on Point-based Graphics (2005) High-quality Surface Splatting on Today's Gpus",
    "abstract": "Abstract not available",
    "authors": [
      "M. Botsch",
      "A. Sorkine-Hornung",
      "Matthias Zwicker",
      "L. Kobbelt"
    ],
    "year": 0,
    "citation_count": 237,
    "arxiv_id": null,
    "doi": null,
    "url": "https://www.semanticscholar.org/paper/081d63ad97b53cb46ed67f17da7a7475f65a88d0",
    "venue": "",
    "publication_date": null,
    "relevance_score": 50,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Representing Scenes as Neural Radiance Fields for View Synthesis",
    "abstract": "Abstract not available",
    "authors": [],
    "year": 0,
    "citation_count": 4973,
    "arxiv_id": null,
    "doi": null,
    "url": "https://www.semanticscholar.org/paper/5410086e3c7cb4cc48f73f75e843b4109befe092",
    "venue": "",
    "publication_date": null,
    "relevance_score": 50,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "Scalable neural indoor scene rendering",
    "abstract": "We propose a scalable neural scene reconstruction and rendering method to support distributed training and interactive rendering of large indoor scenes. Our representation is based on tiles. Tile appearances are trained in parallel through a background sampling strategy that augments each tile with distant scene information via a proxy global mesh. Each tile has two low-capacity MLPs: one for view-independent appearance (diffuse color and shading) and one for view-dependent appearance (specular highlights, reflections). We leverage the phenomena that complex view-dependent scene reflections can be attributed to virtual lights underneath surfaces at the total ray distance to the source. This lets us handle sparse samplings of the input scene where reflection highlights do not always appear consistently in input images. We show interactive free-viewpoint rendering results from five scenes, one of which covers an area of more than 100 m2. Experimental results show that our method produces higher-quality renderings than a single large-capacity MLP and five recent neural proxy-geometry and voxel-based baseline methods. Our code and data are available at project webpage https://xchaowu.github.io/papers/scalable-nisr.",
    "authors": [
      "Xiuchao Wu",
      "Jiamin Xu",
      "Zihan Zhu",
      "H. Bao",
      "Qi-Xing Huang",
      "James Tompkin",
      "Weiwei Xu"
    ],
    "year": 2022,
    "citation_count": 46,
    "arxiv_id": null,
    "doi": "10.1145/3528223.3530153",
    "url": "https://www.semanticscholar.org/paper/8f16b0fcce850ba78270c2088cba64425524db2e",
    "venue": "ACM Transactions on Graphics",
    "publication_date": "2022-07-01",
    "relevance_score": 48,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  },
  {
    "title": "VoGE: A Differentiable Volume Renderer using Gaussian Ellipsoids for Analysis-by-Synthesis",
    "abstract": "The Gaussian reconstruction kernels have been proposed by Westover (1990) and studied by the computer graphics community back in the 90s, which gives an alternative representation of object 3D geometry from meshes and point clouds. On the other hand, current state-of-the-art (SoTA) differentiable renderers, Liu et al. (2019), use rasterization to collect triangles or points on each image pixel and blend them based on the viewing distance. In this paper, we propose VoGE, which utilizes the volumetric Gaussian reconstruction kernels as geometric primitives. The VoGE rendering pipeline uses ray tracing to capture the nearest primitives and blends them as mixtures based on their volume density distributions along the rays. To efficiently render via VoGE, we propose an approximate closeform solution for the volume density aggregation and a coarse-to-fine rendering strategy. Finally, we provide a CUDA implementation of VoGE, which enables real-time level rendering with a competitive rendering speed in comparison to PyTorch3D. Quantitative and qualitative experiment results show VoGE outperforms SoTA counterparts when applied to various vision tasks, e.g., object pose estimation, shape/texture fitting, and occlusion reasoning. The VoGE library and demos are available at: https://github.com/Angtian/VoGE.",
    "authors": [
      "Angtian Wang",
      "Peng Wang",
      "Jian Sun",
      "Adam Kortylewski",
      "A. Yuille"
    ],
    "year": 2022,
    "citation_count": 25,
    "arxiv_id": "2205.15401",
    "doi": "10.48550/arXiv.2205.15401",
    "url": "https://www.semanticscholar.org/paper/31e79b62a9483dcdf2575603469e6ff888e7f234",
    "venue": "International Conference on Learning Representations",
    "publication_date": "2022-05-30",
    "relevance_score": 42.5,
    "selection_reason": "Referenced by seminal 3DGS paper (foundational work)"
  }
]